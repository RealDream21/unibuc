{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16513cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import HTML\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e595159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In how many slices discretize the continuous space, the bigger, the smoother. but it increases a lot the time to converge !\n",
    "# Try and check to see how they work !\n",
    "numStates_pos = 10\n",
    "numStates_speed = 20\n",
    "numStates = np.array([numStates_pos, numStates_speed])\n",
    "\n",
    "# The environment low, high and interval mapped per state\n",
    "env_low = None\n",
    "env_high = None\n",
    "env_dx = None\n",
    "\n",
    "# Number of episodes\n",
    "numEpisodes = 50000\n",
    "maxStepsPerEpisode = 200 #Number of max actions taken per episode. If in 200 steps it's not done, the environment takes it as fail.\n",
    "\n",
    "# Tweaking params\n",
    "initial_lr = 1.0 #Initial Learning Rate\n",
    "lr_decay = 0.999\n",
    "min_lr = 0.001 #Minimum Learning Rate\n",
    "gamma = 1.0 #Discount factor\n",
    "epsilon_start = 1.0 # Allow the model to do a lot of trial and error on the beggining\n",
    "epsilon_decay = 0.999 # Decay per episode.\n",
    "epsilon_end = 0.01 # The end point / min of the epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c92d7257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get epsilon by Episode\n",
    "def get_epsilon(n_episode):\n",
    "    epsilon = max(epsilon_start * (epsilon_decay ** n_episode), epsilon_end)\n",
    "    return (epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0670789e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_frames_as_gif(frames):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a gif, with controls\n",
    "    \"\"\"\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    display(HTML(anim.to_jshtml()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1711f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obs_to_state(env, obs):\n",
    "    \"\"\" Maps an observation to state \"\"\"\n",
    "    if (type(obs) is tuple):\n",
    "        obs = obs[0]\n",
    "\n",
    "    position = int((obs[0] - env_low[0])/env_dx[0])# obs[0] = [-1.2, 0.6]\n",
    "    speed = int((obs[1] - env_low[1])/env_dx[1])# obs[1] = [-0.07, 0.07]\n",
    "    return position, speed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a0e88ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, policy=None, render=False):\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    frames = []\n",
    "    for _ in range(maxStepsPerEpisode):\n",
    "        if render:\n",
    "            frames.append(env.render())\n",
    "        if policy is None:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            pos, speed = obs_to_state(env, obs)\n",
    "            action = policy[pos][speed]\n",
    "        obs, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if render:\n",
    "        #env.render()\n",
    "        display_frames_as_gif(frames)\n",
    "        env.close()\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bbb2ba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_learning(env):\n",
    "    print('Start Q-Learning training:')\n",
    "    display_freq = min(numEpisodes // 10, 1000)\n",
    "\n",
    "    # Initialize Q-Table\n",
    "    q_table = np.random.uniform(-1, 1, (numStates[0], numStates[1], 3))  # [number_of_positions x number_of_speeds x number_of_actionst]\n",
    "    last100_moving_total = 0\n",
    "    last100_rewards = deque()\n",
    "    SOLVED = False\n",
    "    last_total_rewards = [] # For stat purposes, accumultates some episode rewards\n",
    "    for i in range(numEpisodes):\n",
    "        epsilon_to_use = get_epsilon(i)\n",
    "        obs = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        ## Learning rate is decreased at each step. Just another version of what you've seen in the previous labs\n",
    "        lr = max(min_lr, initial_lr * (lr_decay ** i))\n",
    "\n",
    "        for j in range(maxStepsPerEpisode):\n",
    "            pos, speed = obs_to_state(env, obs)  # Get action,state to pick from Q-Table\n",
    "\n",
    "            if np.random.uniform(0, 1) < epsilon_to_use:  # Randomize sometimes\n",
    "                action = np.random.choice(env.action_space.n)\n",
    "            else:\n",
    "                # Q-Table picking process\n",
    "                logits = q_table[pos][speed]  # [lista actiuni] [1,2,3]\n",
    "                logits_exp = np.exp(logits)\n",
    "                probs = logits_exp / np.sum(logits_exp)\n",
    "                action = np.random.choice(env.action_space.n, p=probs)\n",
    "\n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            # Update Q-Table\n",
    "            pos_next, speed_next = obs_to_state(env, obs)\n",
    "            target = reward + gamma * np.max(q_table[pos_next][speed_next])\n",
    "            q_table[pos][speed][action] = q_table[pos][speed][action] + lr * (target - q_table[pos][speed][action])\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        last100_rewards.append(total_reward)\n",
    "        last100_moving_total += total_reward\n",
    "        while len(last100_rewards) > 100:\n",
    "            removedItem = last100_rewards.popleft()\n",
    "            last100_moving_total -= removedItem\n",
    "        last100_moving_avg = last100_moving_total / len(last100_rewards)\n",
    "        if len(last100_rewards) >= 100 and last100_moving_avg >= -180:\n",
    "            print(f\"We solved the game at episode {i} !\")\n",
    "            SOLVED = True\n",
    "            break\n",
    "\n",
    "\n",
    "        if i % display_freq == 0:  # Write out partial results\n",
    "            print(f'At episode: {i+1} - Reward mean from last 100 episodes: {last100_moving_avg}. - LR:{lr:0.4f} - eps:{epsilon_to_use:0.4f}')\n",
    "            last_total_rewards.clear()\n",
    "\n",
    "    print('Training finished!')\n",
    "    solution_policy = np.argmax(q_table, axis=2)\n",
    "    solution_policy_scores = [run_episode(env, solution_policy, False) for _ in range(1000)]\n",
    "    print(\"Average score of solution on a dry run= \", np.mean(solution_policy_scores))\n",
    "\n",
    "    return solution_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a16b74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'MountainCar-v0'\n",
    "env = gym.make(env_name, render_mode='rgb_array')\n",
    "\n",
    "# Read the env things\n",
    "env_low = env.observation_space.low\n",
    "env_high = env.observation_space.high\n",
    "env_dx = (env_high - env_low) / numStates\n",
    "\n",
    "# Train a policy. TODO: save it\n",
    "sol_policy = train_q_learning(env)\n",
    "\n",
    "\n",
    "# Play  simulation with the learned policy\n",
    "run_episode(env, sol_policy, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
