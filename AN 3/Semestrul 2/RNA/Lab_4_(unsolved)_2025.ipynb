{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjZTLbz4Vx4c"
      },
      "source": [
        "<font size=25>Laboratory 4 summary</font>\n",
        "\n",
        "In this lab you will gain debugging experience by solving the most typical deep learning bugs.\n",
        "\n",
        "There are 13 exercises, each one with a corresponding cell. Run the cell, inspect the error and fix the code.\n",
        "\n",
        "Tips:\n",
        " - the bugs can be fixed in several lines of code (usually one or two)\n",
        " - some code in the sections must not be modified and is clearly delimited with comments\n",
        " - try not to inspect other exercises while solving the current one"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUoo-4E12opk"
      },
      "source": [
        "# **Exercises**\n",
        "\n",
        "Run the cell below to import the packages, which are required for all the exercises below."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function, division\n",
        "import os\n",
        "import torch\n",
        "import random\n",
        "from typing import Iterator, List, Callable, Tuple\n",
        "from functools import partial\n",
        "import warnings\n",
        "from math import *\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# Numpy\n",
        "import numpy as np\n",
        "# Pandas\n",
        "import pandas as pd\n",
        "\n",
        "# PyTorch packages\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
        "from torch.utils.data import RandomSampler, Sampler\n",
        "from torchvision import transforms, utils, datasets\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# matplotlib\n",
        "from matplotlib import rc, cm\n",
        "rc('animation', html='jshtml')\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits import mplot3d\n",
        "import matplotlib.animation as animation\n",
        "%matplotlib notebook\n",
        "#warnings.filterwarnings(\"ignore\")\n",
        "plt.ion()   # interactive mode"
      ],
      "metadata": {
        "id": "8vvjpuXSFmFD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "261dd9fa-c3e3-4b9b-8440-26eeae5046f4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<contextlib.ExitStack at 0x79a2fbaabc90>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsINHO_NwDv5"
      },
      "source": [
        "## Exercise 1: Getting started\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_size: int,\n",
        "                 hidden_size: int,\n",
        "                 activation_fn: Callable):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size #100\n",
        "        self.hidden_size = hidden_size #256\n",
        "        self.hidden_layer = nn.Linear(input_size, hidden_size) #(100, 256)\n",
        "        self.output_layer = nn.Linear(hidden_size, 2) # 256\n",
        "        self.activation_fn = activation_fn # relu\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.hidden_layer(x)\n",
        "        h = self.activation_fn(h)\n",
        "        out = self.output_layer(h)\n",
        "\n",
        "        return out\n",
        "\n",
        "# DO NOT MODIFY MODEL INSTANTIATION BELOW\n",
        "#####################################################################\n",
        "model = MLP(input_size=100, hidden_size=256, activation_fn=nn.ReLU())\n",
        "#####################################################################\n",
        "\n",
        "x = torch.rand(32, 100)\n",
        "\n",
        "y = model(x)\n",
        "assert y.shape[0] == 32 and y.shape[1] == 2, \"Wrong output shape\""
      ],
      "metadata": {
        "id": "k1BvB8TwEPzx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Getting in shape"
      ],
      "metadata": {
        "id": "uHW1o4myBc4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_size: int, #784\n",
        "                 hidden_size: int, #256\n",
        "                 activation_fn: Callable): #relu\n",
        "        super().__init__()\n",
        "        self.input_size = input_size #784\n",
        "        self.hidden_size = hidden_size #256\n",
        "        self.hidden_layer = nn.Linear(input_size, hidden_size) #(784, 256)\n",
        "        self.output_layer = nn.Linear(hidden_size, 2) # (512, 2)\n",
        "        self.flatten_layer = nn.Flatten()\n",
        "        self.activation_fn = activation_fn\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.flatten_layer(x)\n",
        "        h = self.hidden_layer(h)\n",
        "        h = self.activation_fn(h)\n",
        "        out = self.output_layer(h)\n",
        "\n",
        "        return out\n",
        "\n",
        "# DO NOT MODIFY MODEL INSTANTIATION BELOW\n",
        "#####################################################################\n",
        "model = MLP(input_size=784, hidden_size=256, activation_fn=nn.ReLU())\n",
        "#####################################################################\n",
        "\n",
        "# download MNIST dataset\n",
        "mnist_trainset = datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor())\n",
        "\n",
        "# select 10th example\n",
        "x, l = mnist_trainset[10]\n",
        "\n",
        "y = model(x)\n",
        "\n",
        "assert y.shape[0] == 1 and y.shape[1] == 2, \"Wrong output shape\""
      ],
      "metadata": {
        "id": "3GCJ9iNHNFIe"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3: It's the little things\n"
      ],
      "metadata": {
        "id": "3beV3q_MBf4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_size: int,\n",
        "                 hidden_size: int,\n",
        "                 activation_fn: Callable): #relu\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.hidden_layer = nn.Linear(input_size, hidden_size)\n",
        "        self.output_layer = nn.Linear(hidden_size, 2)\n",
        "        self.activation_fn = activation_fn\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.hidden_layer(x)\n",
        "        h = self.activation_fn(h)\n",
        "        out = self.output_layer(h)\n",
        "\n",
        "        return out\n",
        "\n",
        "model = MLP(input_size=784, hidden_size=256, activation_fn=nn.ReLU())\n",
        "\n",
        "x = torch.rand(32, 784)\n",
        "y = model(x)\n",
        "assert y.shape[0] == 32 and y.shape[1] == 2, \"Wrong output shape\""
      ],
      "metadata": {
        "id": "ijj1R6SXP-fc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 4: No one left behind"
      ],
      "metadata": {
        "id": "QAJtebM_Bjck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_size: int, #784\n",
        "                 hidden_size: int,#256\n",
        "                 batch_size: int, #BATCH_SIZE\n",
        "                 activation_fn: Callable): #ReLU()\n",
        "        super().__init__()\n",
        "        self.input_size = input_size #784\n",
        "        self.hidden_size = hidden_size #256\n",
        "        self.hidden_layer = nn.Linear(input_size, hidden_size) #(784, 256)\n",
        "        self.output_layer = nn.Linear(hidden_size, 10) #(256, 10)\n",
        "        self.batch_size = batch_size\n",
        "        self.activation_fn = activation_fn\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input x has shape: batch_size x 1 x 28 x 28\n",
        "        # we resize it to:   batch_size x 784\n",
        "        x = x.view(-1, 784)\n",
        "\n",
        "        h = self.hidden_layer(x)\n",
        "        h = self.activation_fn(h)\n",
        "        out = self.output_layer(h)\n",
        "\n",
        "        return out\n",
        "\n",
        "# DO NOT MODIFY HYPERPARAMETERS AND MODEL INSTANTIATION BELOW\n",
        "#############################################################\n",
        "BATCH_SIZE=32\n",
        "model = MLP(\n",
        "    input_size=784,\n",
        "    hidden_size=256,\n",
        "    activation_fn=nn.ReLU(),\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "##############################################################\n",
        "\n",
        "# instantiate MNIST dataset\n",
        "val_dataset = datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor())\n",
        "print(\"validation dataset size = \", len(val_dataset))\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "loss_crt = nn.CrossEntropyLoss()\n",
        "epoch_loss = 0.0\n",
        "for batch_images, batch_labels in val_dataloader:\n",
        "    # batch_size x 2\n",
        "    out = model(batch_images)\n",
        "    loss = loss_crt(out, batch_labels)\n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "epoch_loss /= len(val_dataloader)\n",
        "print(\"Validation loss = \", epoch_loss)"
      ],
      "metadata": {
        "id": "cbXjL4t37_JZ",
        "outputId": "031c6302-67e1-4ac8-8d16-d203901e83ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation dataset size =  10000\n",
            "Validation loss =  2.3022900694094526\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 5: Left to their own devices\n"
      ],
      "metadata": {
        "id": "BvVmqB1tHSF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_size: int,\n",
        "                 hidden_size: int,\n",
        "                 batch_size: int,\n",
        "                 device: torch.device,\n",
        "                 activation_fn: Callable):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.hidden_layer = nn.Linear(input_size, hidden_size)\n",
        "        self.output_layer = nn.Linear(hidden_size, 10)\n",
        "        self.activation_fn = activation_fn\n",
        "        self.device = device\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        # move Tensor to GPU (if available)\n",
        "        x = x.to(self.device)\n",
        "\n",
        "        # reshape tensor\n",
        "        # batch_size x 784\n",
        "        x = x.view(self.batch_size, -1)\n",
        "\n",
        "        h = self.hidden_layer(x)\n",
        "        h = self.activation_fn(h)\n",
        "        out = self.output_layer(h)\n",
        "\n",
        "        return out\n",
        "\n",
        "# DO NOT MODIFY DEVICE TENSOR BELOW\n",
        "#################################################################################\n",
        "BATCH_SIZE=32\n",
        "device = torch.device('cuda') if torch.cuda.is_available else torch.device('cpu')\n",
        "print(\"device = \", device)\n",
        "#################################################################################\n",
        "\n",
        "\n",
        "# instantiate model\n",
        "model = MLP(\n",
        "    input_size=784, hidden_size=256, activation_fn=nn.ReLU(), batch_size=32,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# move model to GPU (Module.to() is an in-place operation, it recursively\n",
        "# processes parameters inside your nn.Module)\n",
        "model.to(device)\n",
        "\n",
        "# instantiate MNIST dataset\n",
        "train_dataset = datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor())\n",
        "print(\"train dataset size = \", len(train_dataset))\n",
        "\n",
        "# instantiate dataloader\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "loss_crt = nn.CrossEntropyLoss()\n",
        "epoch_loss = 0.0\n",
        "for batch_images, batch_labels in train_dataloader:\n",
        "    # move labels to GPU (if available)\n",
        "    batch_labels = batch_labels.to(device)\n",
        "\n",
        "    # batch_size x 2\n",
        "    # feedforward\n",
        "    out = model(batch_images)\n",
        "\n",
        "    # compute loss\n",
        "    loss = loss_crt(out, batch_labels)\n",
        "    epoch_loss += loss.item()"
      ],
      "metadata": {
        "id": "bymjthPlMBY3",
        "outputId": "1fbcc31a-f1af-4421-d792-74b0994b3029",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device =  cuda\n",
            "train dataset size =  60000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 6: Not exactly my type"
      ],
      "metadata": {
        "id": "6xEcE46Y9q74"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task I"
      ],
      "metadata": {
        "id": "ixW0tkSJ9dTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_size: int,\n",
        "                 hidden_size: int,\n",
        "                 batch_size: int,\n",
        "                 activation_fn: Callable):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.hidden_layer = nn.Linear(input_size, hidden_size)\n",
        "        self.output_layer = nn.Linear(hidden_size, 10)\n",
        "        self.activation_fn = activation_fn\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        # reshape tensor\n",
        "        # batch_size x 784\n",
        "        x = x.view(self.batch_size, -1)\n",
        "\n",
        "        h = self.hidden_layer(x)\n",
        "        h = self.activation_fn(h)\n",
        "        out = self.output_layer(h)\n",
        "\n",
        "        return out\n",
        "\n",
        "BATCH_SIZE=32\n",
        "\n",
        "# instantiate model\n",
        "model = MLP(\n",
        "    input_size=784, hidden_size=256, activation_fn=nn.ReLU(), batch_size=32\n",
        ")\n",
        "\n",
        "# instantiate MNIST dataset\n",
        "train_dataset = datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor()\n",
        ")\n",
        "print(\"train dataset size = \", len(train_dataset))\n",
        "\n",
        "# instantiate dataloader\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "loss_crt = nn.CrossEntropyLoss()\n",
        "epoch_loss = 0.0\n",
        "for batch_images, batch_labels in train_dataloader:\n",
        "    # batch_size x 2\n",
        "    # feedforward\n",
        "    out = model(batch_images)\n",
        "\n",
        "    # compute loss\n",
        "    loss = loss_crt(out, batch_labels)\n",
        "    epoch_loss += loss.item()"
      ],
      "metadata": {
        "id": "uqhsW2M3-B1T",
        "outputId": "5711c4c4-e9ed-4d52-b02f-58aa716ea238",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train dataset size =  60000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task II"
      ],
      "metadata": {
        "id": "OSXSMtDM-Pz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_size: int,\n",
        "                 hidden_size: int,\n",
        "                 device: torch.device,\n",
        "                 activation_fn: Callable):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.hidden_layer = nn.Linear(input_size, hidden_size)\n",
        "        self.output_layer = nn.Linear(hidden_size, 10)\n",
        "        self.activation_fn = activation_fn\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, x):\n",
        "        # batch_size x 64\n",
        "        x = x.to(self.device)\n",
        "\n",
        "        h = self.hidden_layer(x)\n",
        "        h = self.activation_fn(h)\n",
        "        out = self.output_layer(h)\n",
        "\n",
        "        return out\n",
        "\n",
        "BATCH_SIZE=32\n",
        "device = torch.device('cuda') if torch.cuda.is_available else torch.device('cpu')\n",
        "print(\"device = \", device)\n",
        "\n",
        "# instantiate model\n",
        "model = MLP(\n",
        "    input_size=64, hidden_size=256, activation_fn=nn.ReLU(), device=device\n",
        ")\n",
        "\n",
        "# move model to GPU (Module.to() is an in-place operation)\n",
        "model.to(device)\n",
        "\n",
        "# load the 1797 images from the Digits dataset:\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html\n",
        "# Images are grayscale digits from 0 to 9, stored as arrays of size 64 (8x8).\n",
        "# Both images and labels are stored are NumPy arrays, so we need to convert\n",
        "# them to Tensors.\n",
        "x = load_digits()\n",
        "\n",
        "# 1797 x 64, 1797\n",
        "images, labels = torch.tensor(x.data), torch.tensor(x.target)\n",
        "\n",
        "# we create a TensorDataset, which is a type of Dataset that wraps Tensors.\n",
        "# https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset\n",
        "# Examples are indexed over the first dimension, so the first dimension of\n",
        "# the Tensors must be the same (1797 in our case)\n",
        "train_dataset = TensorDataset(images, labels)\n",
        "\n",
        "# instantiate dataloader\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "loss_crt = nn.CrossEntropyLoss()\n",
        "epoch_loss = 0.0\n",
        "for batch_images, batch_labels in train_dataloader:\n",
        "    batch_labels=batch_labels.to(device)\n",
        "\n",
        "    batch_images = batch_images.float()\n",
        "    # batch_size x 2\n",
        "    # feedforward\n",
        "    out = model(batch_images)\n",
        "\n",
        "    # compute loss\n",
        "    loss = loss_crt(out, batch_labels)\n",
        "    epoch_loss += loss.item()"
      ],
      "metadata": {
        "id": "WDdeFJtSW0ea",
        "outputId": "7bffe1a3-d034-4a4f-c5b6-db7807b2f365",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device =  cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 7: Out of bounds\n",
        "The [Wheat Seeds](https://archive.ics.uci.edu/ml/datasets/seeds) dataset ([Kaggle link](https://www.kaggle.com/jmcaro/wheat-seedsuci)) is a classification task with 3 classes, which contains 209 examples. Each example contains 7 geometrical properties of wheat seeds belonging to 3 varieties of wheat.\n",
        "\n",
        "**Hint 1:** When training on GPUs, CUDA errors may be less helpful. Usually, errors such as \"`RuntimeError: CUDA error: device-side assert triggered`\" indicate a problem with an index, which may be too large. To get a more accurate error message, move the model and dataset to CPU, check the error again and try to fix it.\n",
        "\n",
        "**Hint 2:** After fixing the code responsible for a CUDA error, you may still encounter the error when running on GPU. Try restarting the Colab Notebook (`Runtime` -> `Restart runtime`) and run the cells again.\n"
      ],
      "metadata": {
        "id": "1WRHhMa4nX8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_size: int,\n",
        "                 hidden_size: int,\n",
        "                 device: torch.device,\n",
        "                 activation_fn: Callable):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.hidden_layer = nn.Linear(input_size, hidden_size)\n",
        "        self.output_layer = nn.Linear(hidden_size, 3)\n",
        "        self.activation_fn = activation_fn\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, x):\n",
        "        # batch_size x 7\n",
        "        x = x.to(self.device)\n",
        "\n",
        "        h = self.hidden_layer(x)\n",
        "        h = self.activation_fn(h)\n",
        "        out = self.output_layer(h)\n",
        "\n",
        "        return out\n",
        "\n",
        "BATCH_SIZE=32\n",
        "\n",
        "# if you encounter a vague CUDA error message, move the operations to CPU then\n",
        "# run the code again. The error message is usually more helpful.\n",
        "device = torch.device('cpu') if torch.cuda.is_available else torch.device('cpu')\n",
        "print(\"device = \", device)\n",
        "\n",
        "# instantiate model\n",
        "model = MLP(\n",
        "    input_size=7, hidden_size=128, activation_fn=nn.ReLU(), device=device\n",
        ")\n",
        "\n",
        "# move model to GPU (Module.to() is an in-place operation)\n",
        "model.to(device)\n",
        "\n",
        "# download Wheat Seeds dataset\n",
        "!wget --no-check-certificate \\\n",
        "https://raw.githubusercontent.com/jbrownlee/Datasets/master/wheat-seeds.csv \\\n",
        "-O /tmp/wheat.csv\n",
        "\n",
        "# read Wheat Seeds dataset from csv\n",
        "# Dataset has 209 examples. Each example has 7 attributes (features).\n",
        "# It's a classification task with 3 classes (1, 2 and 3)\n",
        "data = pd.read_csv(\"/tmp/wheat.csv\")\n",
        "\n",
        "# put examples in a Tensor\n",
        "x = torch.tensor(data.values, dtype=torch.float32)\n",
        "\n",
        "# separate data and labels\n",
        "data, labels = x[:,:-1], x[:,-1].long()\n",
        "\n",
        "# we create a TensorDataset, which is a type of Dataset that wraps Tensors.\n",
        "# https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset\n",
        "# Examples are indexed over the first dimension, so the first dimension of\n",
        "# the Tensors must be the same (209 in our case)\n",
        "validation_dataset = TensorDataset(data, labels)\n",
        "\n",
        "# instantiate dataloader\n",
        "validation_dataloader = DataLoader(\n",
        "    validation_dataset,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "loss_crt = nn.CrossEntropyLoss()\n",
        "epoch_loss = 0.0\n",
        "for batch_images, batch_labels in validation_dataloader:\n",
        "    batch_labels=batch_labels.to(device)\n",
        "\n",
        "    # feedforward\n",
        "    # batch_size x 3\n",
        "    batch_labels -= 1\n",
        "    out = model(batch_images)\n",
        "\n",
        "    # compute loss\n",
        "    loss = loss_crt(out, batch_labels)#out is size(32, 3), labels is size(32, )\n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "epoch_loss /= len(validation_dataloader)\n",
        "print(\"Validation loss = \", epoch_loss)"
      ],
      "metadata": {
        "id": "xZP5yCbeo5Pd",
        "outputId": "ae257d3a-a413-4899-a6eb-419249ef4c3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device =  cpu\n",
            "--2025-03-19 15:04:40--  https://raw.githubusercontent.com/jbrownlee/Datasets/master/wheat-seeds.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9301 (9.1K) [text/plain]\n",
            "Saving to: ‘/tmp/wheat.csv’\n",
            "\n",
            "\r/tmp/wheat.csv        0%[                    ]       0  --.-KB/s               \r/tmp/wheat.csv      100%[===================>]   9.08K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-03-19 15:04:40 (81.5 MB/s) - ‘/tmp/wheat.csv’ saved [9301/9301]\n",
            "\n",
            "Validation loss =  3.2895468888538226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 8: I have no memory of that\n",
        "\n",
        "**Hint 1:** The error will appear after ~1 epoch\n",
        "\n",
        "**Hint 2:** You do NOT need to modify the model's size to fix the memory bug\n",
        "\n",
        "**Hint 3:** After getting the error message, you have to restart the machine:\n",
        "  - restart Colab: `Runtime` -> `Restart runtime`\n",
        "  - run the cell that imports packages\n",
        "  - run the cell below"
      ],
      "metadata": {
        "id": "4v5D0k3MHsW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_size: int,\n",
        "                 hidden_size_1: int,\n",
        "                 hidden_size_2: int,\n",
        "                 hidden_size_3: int,\n",
        "                 hidden_size_4: int,\n",
        "                 device: torch.device,\n",
        "                 activation_fn: Callable):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size_1 = hidden_size_1\n",
        "        self.hidden_size_2 = hidden_size_2\n",
        "        self.hidden_layer_1 = nn.Linear(input_size, hidden_size_1)\n",
        "        self.hidden_layer_2 = nn.Linear(hidden_size_1, hidden_size_2)\n",
        "        self.hidden_layer_3 = nn.Linear(hidden_size_2, hidden_size_3)\n",
        "        self.hidden_layer_4 = nn.Linear(hidden_size_3, hidden_size_4)\n",
        "        self.output_layer = nn.Linear(hidden_size_4, 10)\n",
        "        self.activation_fn = activation_fn\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, x):\n",
        "        # move input data to GPU (if available)\n",
        "        x = x.to(self.device)\n",
        "\n",
        "        # reshape tensor\n",
        "        # batch_size x 784\n",
        "        x = x.view(-1, self.input_size)\n",
        "\n",
        "        h1 = self.activation_fn(self.hidden_layer_1(x))\n",
        "        h2 = self.activation_fn(self.hidden_layer_2(h1))\n",
        "        h3 = self.activation_fn(self.hidden_layer_3(h2))\n",
        "        h4 = self.activation_fn(self.hidden_layer_4(h3))\n",
        "        out = self.output_layer(h4)\n",
        "\n",
        "        return out\n",
        "\n",
        "BATCH_SIZE=32\n",
        "device = torch.device('cuda') if torch.cuda.is_available else torch.device('cpu')\n",
        "print(\"device = \", device)\n",
        "\n",
        "# DO NOT MODIFY MODEL INSTANTIATION BELOW\n",
        "#########################################\n",
        "model = MLP(\n",
        "    input_size=784,\n",
        "    hidden_size_1=16384,\n",
        "    hidden_size_2=16384,\n",
        "    hidden_size_3=16384,\n",
        "    hidden_size_4=16384,\n",
        "    activation_fn=nn.ReLU(),\n",
        "    device=device\n",
        ")\n",
        "#########################################\n",
        "\n",
        "# move model to GPU (Module.to() is an in-place operation)\n",
        "model.to(device)\n",
        "\n",
        "# instantiate MNIST dataset\n",
        "train_dataset = datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor())\n",
        "print(\"train dataset size = \", len(train_dataset))\n",
        "\n",
        "# instantiate dataloader\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "loss_crt = nn.CrossEntropyLoss()\n",
        "epoch_loss = 0.0\n",
        "num_batches = len(train_dataloader)\n",
        "for epoch in range(20):\n",
        "    for idx, (batch_images, batch_labels) in enumerate(train_dataloader):\n",
        "        if idx % 50 == 0:\n",
        "            print(\"epoch %d, batch %d/%d\" % (epoch, idx, num_batches))\n",
        "\n",
        "        # move labels to GPU (if available)\n",
        "        batch_labels=batch_labels.to(device)\n",
        "\n",
        "        # batch_size x 2\n",
        "        # feedforward\n",
        "        out = model(batch_images)\n",
        "\n",
        "        # compute loss\n",
        "        loss = loss_crt(out, batch_labels)\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        del batch_images\n",
        "        del batch_labels\n",
        "\n",
        "    epoch_loss /= num_batches\n",
        "    print(\"epoch loss = \", epoch_loss)"
      ],
      "metadata": {
        "id": "mAahmqrJae1h",
        "outputId": "6a7172c8-4db5-44c5-8298-7c36469302b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device =  cuda\n",
            "train dataset size =  60000\n",
            "epoch 0, batch 0/1875\n",
            "epoch 0, batch 50/1875\n",
            "epoch 0, batch 100/1875\n",
            "epoch 0, batch 150/1875\n",
            "epoch 0, batch 200/1875\n",
            "epoch 0, batch 250/1875\n",
            "epoch 0, batch 300/1875\n",
            "epoch 0, batch 350/1875\n",
            "epoch 0, batch 400/1875\n",
            "epoch 0, batch 450/1875\n",
            "epoch 0, batch 500/1875\n",
            "epoch 0, batch 550/1875\n",
            "epoch 0, batch 600/1875\n",
            "epoch 0, batch 650/1875\n",
            "epoch 0, batch 700/1875\n",
            "epoch 0, batch 750/1875\n",
            "epoch 0, batch 800/1875\n",
            "epoch 0, batch 850/1875\n",
            "epoch 0, batch 900/1875\n",
            "epoch 0, batch 950/1875\n",
            "epoch 0, batch 1000/1875\n",
            "epoch 0, batch 1050/1875\n",
            "epoch 0, batch 1100/1875\n",
            "epoch 0, batch 1150/1875\n",
            "epoch 0, batch 1200/1875\n",
            "epoch 0, batch 1250/1875\n",
            "epoch 0, batch 1300/1875\n",
            "epoch 0, batch 1350/1875\n",
            "epoch 0, batch 1400/1875\n",
            "epoch 0, batch 1450/1875\n",
            "epoch 0, batch 1500/1875\n",
            "epoch 0, batch 1550/1875\n",
            "epoch 0, batch 1600/1875\n",
            "epoch 0, batch 1650/1875\n",
            "epoch 0, batch 1700/1875\n",
            "epoch 0, batch 1750/1875\n",
            "epoch 0, batch 1800/1875\n",
            "epoch 0, batch 1850/1875\n",
            "epoch loss =  2.303647885386149\n",
            "epoch 1, batch 0/1875\n",
            "epoch 1, batch 50/1875\n",
            "epoch 1, batch 100/1875\n",
            "epoch 1, batch 150/1875\n",
            "epoch 1, batch 200/1875\n",
            "epoch 1, batch 250/1875\n",
            "epoch 1, batch 300/1875\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-8871a263680f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_crt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mbatch_images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 9: Underground\n",
        "\n",
        "Validation accuracy on CIFAR10 with this simple MLP should reach ~48%. However, there is a bug preventing that from happening.\n",
        "\n",
        "**Hint**: Inspect the training and validation losses."
      ],
      "metadata": {
        "id": "o_XMAAIA9r0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_size: int,\n",
        "                 hidden_size: int,\n",
        "                 device: torch.device,\n",
        "                 activation_fn: Callable,\n",
        "                 output_activation_fn: Callable):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.hidden_layer = nn.Linear(input_size, hidden_size)\n",
        "        self.output_layer = nn.Linear(hidden_size, 10)\n",
        "        self.activation_fn = activation_fn\n",
        "        self.output_activation_fn = output_activation_fn\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, x):\n",
        "        # move input data to GPU (if available)\n",
        "        x = x.to(self.device)\n",
        "\n",
        "        # reshape tensor\n",
        "        # batch_size x 784\n",
        "        batch_size = x.shape[0]\n",
        "        x = x.view(batch_size, -1)\n",
        "\n",
        "        h = self.hidden_layer(x)\n",
        "        h = self.activation_fn(h)\n",
        "        out = self.output_layer(h)\n",
        "        out = self.output_activation_fn(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "BATCH_SIZE=128\n",
        "NUM_EPOCHS=20\n",
        "device = torch.device('cuda') if torch.cuda.is_available else torch.device('cpu')\n",
        "print(\"device = \", device)\n",
        "\n",
        "model = MLP(\n",
        "    input_size=3072, hidden_size=1024, activation_fn=nn.ReLU(), device=device,\n",
        "    output_activation_fn=nn.Softmax(dim=0)\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "# instantiate MNIST train and validation datasets\n",
        "train_dataset = datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor()\n",
        ")\n",
        "val_dataset = datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor()\n",
        ")\n",
        "print(\"train dataset size = \", len(train_dataset))\n",
        "print(\"validation dataset size = \", len(val_dataset))\n",
        "\n",
        "# instantiate dataloaders\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=4\n",
        ")\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=4\n",
        ")\n",
        "num_train_batches = len(train_dataloader)\n",
        "num_val_batches = len(val_dataloader)\n",
        "\n",
        "epoch_loss = 0.0\n",
        "train_losses, val_losses = [], []\n",
        "train_predictions, val_predictions = [], []\n",
        "train_labels, val_labels = [], []\n",
        "train_accuracies, val_accuracies = [], []\n",
        "\n",
        "# DO NOT MODIFY LOSS FUNCTION BELOW\n",
        "##############################################################################\n",
        "loss_crt = nn.NLLLoss()\n",
        "##############################################################################\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
        "for epoch_idx in range(NUM_EPOCHS):\n",
        "    train_epoch_loss = 0.0\n",
        "    model.train()\n",
        "    for batch_images, batch_labels in train_dataloader:\n",
        "        model.zero_grad()\n",
        "        batch_labels = batch_labels.to(device)\n",
        "\n",
        "        # feedforward\n",
        "        # batch_size x 10\n",
        "        out = model(batch_images)\n",
        "\n",
        "        batch_predictions = torch.argmax(out, dim=1)\n",
        "        train_predictions += batch_predictions.tolist()\n",
        "        train_labels += batch_labels.tolist()\n",
        "\n",
        "        # compute loss\n",
        "        loss = loss_crt(out, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_epoch_loss += loss.item()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        model.zero_grad()\n",
        "        val_epoch_loss = 0.0\n",
        "        for batch_images, batch_labels in val_dataloader:\n",
        "            batch_labels = batch_labels.to(device)\n",
        "\n",
        "            # batch_size x 10\n",
        "            # feedforward\n",
        "            out = model(batch_images)\n",
        "            batch_predictions = torch.argmax(out, dim=1)\n",
        "            val_predictions += batch_predictions.tolist()\n",
        "            val_labels += batch_labels.tolist()\n",
        "\n",
        "            # compute loss\n",
        "            loss = loss_crt(out, batch_labels)\n",
        "            val_epoch_loss += loss.item()\n",
        "\n",
        "    train_epoch_loss /= num_train_batches\n",
        "    val_epoch_loss /= num_val_batches\n",
        "    train_losses.append(train_epoch_loss)\n",
        "    val_losses.append(val_epoch_loss)\n",
        "\n",
        "    train_acc = accuracy_score(train_labels, train_predictions)\n",
        "    val_acc = accuracy_score(val_labels, val_predictions)\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_accuracies.append(val_acc)\n",
        "    print(\"epoch %d, train acc=%f, val acc=%f\" % (\n",
        "        epoch_idx,\n",
        "        train_acc,\n",
        "        val_acc\n",
        "    ))\n"
      ],
      "metadata": {
        "id": "GxeGzxyTUX8L",
        "outputId": "f753d76f-aabe-4471-d3b5-c825abe39608",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device =  cuda\n",
            "train dataset size =  50000\n",
            "validation dataset size =  10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, train acc=0.288060, val acc=0.302500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, train acc=0.297280, val acc=0.311200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2, train acc=0.302327, val acc=0.313667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3, train acc=0.306575, val acc=0.315525\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4, train acc=0.310256, val acc=0.315140\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5, train acc=0.311767, val acc=0.316533\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6, train acc=0.314014, val acc=0.317329\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 7, train acc=0.315460, val acc=0.319563\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 8, train acc=0.316798, val acc=0.320244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 9, train acc=0.318242, val acc=0.320860\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 10, train acc=0.319433, val acc=0.322473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 11, train acc=0.320913, val acc=0.323567\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 12, train acc=0.321994, val acc=0.324585\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 13, train acc=0.322931, val acc=0.325607\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 14, train acc=0.323961, val acc=0.325853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 15, train acc=0.324852, val acc=0.326875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 16, train acc=0.326058, val acc=0.327518\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 17, train acc=0.327003, val acc=0.328594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 18, train acc=0.328019, val acc=0.329163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 19, train acc=0.328843, val acc=0.330115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "plt.plot(range(0,len(train_accuracies)), train_accuracies, 'g', label='Training accuracy')\n",
        "plt.plot(range(0,len(train_accuracies)), val_accuracies, 'b', label='Validation accuracy')\n",
        "plt.title('Training and Validation accuracies')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "H3hRIIm8zW5j",
        "outputId": "a04985c6-e9c9-4a13-ea69-90163decc5b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "plt.plot(range(0,len(train_losses)), train_losses, 'g', label='Training loss')\n",
        "plt.plot(range(0,len(train_losses)), val_losses, 'b', label='Validation loss')\n",
        "plt.title('Training and Validation accuracies')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V4UrTT4NEP0u",
        "outputId": "cc4af8f9-318b-4f16-e9ae-be56479e143f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 10: Validation >> Train\n",
        "\n",
        "Train the network and inspect the train and validation accuracy curves. Notice a large gap in accuracy (>5%) throughout the epochs. You have adjust the code below such that:\n",
        " - the accuracy gap between train and validation becomes smaller\n",
        " - validation performance gets better"
      ],
      "metadata": {
        "id": "LrlATQYJXR0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_size: int,\n",
        "                 hidden_size: int,\n",
        "                 device: torch.device,\n",
        "                 activation_fn: Callable,\n",
        "                 dropout_rate: float):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.hidden_layer = nn.Linear(input_size, hidden_size)\n",
        "        self.output_layer = nn.Linear(hidden_size, 10)\n",
        "        self.activation_fn = activation_fn\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, x):\n",
        "        # move input data to GPU (if available)\n",
        "        x = x.to(self.device)\n",
        "\n",
        "        # reshape tensor\n",
        "        # batch_size x 784\n",
        "        batch_size = x.shape[0]\n",
        "        x = x.view(batch_size, -1)\n",
        "\n",
        "        h = self.activation_fn(self.dropout(self.hidden_layer(x)))\n",
        "        out = self.output_layer(h)\n",
        "\n",
        "        return out\n",
        "\n",
        "BATCH_SIZE=128\n",
        "NUM_EPOCHS=20\n",
        "device = torch.device('cuda') if torch.cuda.is_available else torch.device('cpu')\n",
        "print(\"device = \", device)\n",
        "\n",
        "model = MLP(\n",
        "    input_size=3072, hidden_size=1024, activation_fn=nn.ReLU(), device=device,\n",
        "    dropout_rate=0.9\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "# instantiate MNIST train and validation datasets\n",
        "train_dataset = datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor()\n",
        ")\n",
        "val_dataset = datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor()\n",
        ")\n",
        "print(\"train dataset size = \", len(train_dataset))\n",
        "print(\"validation dataset size = \", len(val_dataset))\n",
        "\n",
        "# instantiate dataloaders\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=4\n",
        ")\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=4\n",
        ")\n",
        "num_train_batches = len(train_dataloader)\n",
        "num_val_batches = len(val_dataloader)\n",
        "\n",
        "epoch_loss = 0.0\n",
        "train_losses, val_losses = [], []\n",
        "train_predictions, val_predictions = [], []\n",
        "train_labels, val_labels = [], []\n",
        "train_accuracies, val_accuracies = [], []\n",
        "\n",
        "# DO NOT MODIFY LOSS FUNCTION BELOW\n",
        "##############################################################################\n",
        "loss_crt = nn.CrossEntropyLoss()\n",
        "##############################################################################\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
        "for epoch_idx in range(NUM_EPOCHS):\n",
        "    train_epoch_loss = 0.0\n",
        "    model.train()\n",
        "    for batch_images, batch_labels in train_dataloader:\n",
        "        model.zero_grad()\n",
        "        batch_labels = batch_labels.to(device)\n",
        "\n",
        "        # feedforward\n",
        "        # batch_size x 10\n",
        "        out = model(batch_images)\n",
        "\n",
        "        batch_predictions = torch.argmax(out, dim=1)\n",
        "        train_predictions += batch_predictions.tolist()\n",
        "        train_labels += batch_labels.tolist()\n",
        "\n",
        "        # compute loss\n",
        "        loss = loss_crt(out, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_epoch_loss += loss.item()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        val_epoch_loss = 0.0\n",
        "        for batch_images, batch_labels in val_dataloader:\n",
        "            batch_labels = batch_labels.to(device)\n",
        "\n",
        "            # batch_size x 10\n",
        "            # feedforward\n",
        "            out = model(batch_images)\n",
        "            batch_predictions = torch.argmax(out, dim=1)\n",
        "            val_predictions += batch_predictions.tolist()\n",
        "            val_labels += batch_labels.tolist()\n",
        "\n",
        "            # compute loss\n",
        "            loss = loss_crt(out, batch_labels)\n",
        "            val_epoch_loss += loss.item()\n",
        "\n",
        "    train_epoch_loss /= num_train_batches\n",
        "    val_epoch_loss /= num_val_batches\n",
        "    train_losses.append(train_epoch_loss)\n",
        "    val_losses.append(val_epoch_loss)\n",
        "\n",
        "    train_acc = accuracy_score(train_labels, train_predictions)\n",
        "    val_acc = accuracy_score(val_labels, val_predictions)\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_accuracies.append(val_acc)\n",
        "    print(\"epoch %d, train acc=%f, val acc=%f\" % (\n",
        "        epoch_idx,\n",
        "        train_acc,\n",
        "        val_acc\n",
        "    ))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAenZSaSXUbK",
        "outputId": "b809e02e-ad22-406d-d705-4c4e58201c3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device =  cuda\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "train dataset size =  50000\n",
            "validation dataset size =  10000\n",
            "epoch 0, train acc=0.180120, val acc=0.281100\n",
            "epoch 1, train acc=0.193400, val acc=0.289700\n",
            "epoch 2, train acc=0.201713, val acc=0.297700\n",
            "epoch 3, train acc=0.206990, val acc=0.302050\n",
            "epoch 4, train acc=0.210948, val acc=0.306620\n",
            "epoch 5, train acc=0.214280, val acc=0.310767\n",
            "epoch 6, train acc=0.217106, val acc=0.314529\n",
            "epoch 7, train acc=0.219908, val acc=0.315775\n",
            "epoch 8, train acc=0.222058, val acc=0.319067\n",
            "epoch 9, train acc=0.223774, val acc=0.321100\n",
            "epoch 10, train acc=0.225705, val acc=0.322645\n",
            "epoch 11, train acc=0.227455, val acc=0.324350\n",
            "epoch 12, train acc=0.228638, val acc=0.326254\n",
            "epoch 13, train acc=0.229809, val acc=0.327800\n",
            "epoch 14, train acc=0.230783, val acc=0.329400\n",
            "epoch 15, train acc=0.231689, val acc=0.330331\n",
            "epoch 16, train acc=0.232551, val acc=0.331365\n",
            "epoch 17, train acc=0.233411, val acc=0.332850\n",
            "epoch 18, train acc=0.234337, val acc=0.333758\n",
            "epoch 19, train acc=0.234991, val acc=0.334865\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "plt.plot(range(0,len(train_losses)), train_losses, 'g', label='Training loss')\n",
        "plt.plot(range(0,len(train_losses)), val_losses, 'b', label='Validation loss')\n",
        "plt.title('Training and Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.plot(range(0,len(train_accuracies)), train_accuracies, 'g', label='Training accuracy')\n",
        "plt.plot(range(0,len(train_accuracies)), val_accuracies, 'b', label='Validation accuracy')\n",
        "plt.title('Training and Validation accuracies')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UasL94qEZCnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 11: Blown out of proportions"
      ],
      "metadata": {
        "id": "ChI-AtFA9Mxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_size: int,\n",
        "                 hidden_size: int,\n",
        "                 device: torch.device,\n",
        "                 activation_fn: Callable,\n",
        "                 output_activation_fn: Callable):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.hidden_layer = nn.Linear(input_size, hidden_size)\n",
        "        self.output_layer = nn.Linear(hidden_size, 10)\n",
        "        self.activation_fn = activation_fn\n",
        "        self.output_activation_fn = output_activation_fn\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, x):\n",
        "        # move input data to GPU (if available)\n",
        "        x = x.to(self.device)\n",
        "\n",
        "        # reshape tensor\n",
        "        # batch_size x 784\n",
        "        batch_size = x.shape[0]\n",
        "        x = x.view(batch_size, -1)\n",
        "\n",
        "        h = self.hidden_layer(x)\n",
        "        h = self.activation_fn(h)\n",
        "        out = self.output_layer(h)\n",
        "        out = self.output_activation_fn(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "BATCH_SIZE=128\n",
        "NUM_EPOCHS=20\n",
        "device = torch.device('cuda') if torch.cuda.is_available else torch.device('cpu')\n",
        "print(\"device = \", device)\n",
        "\n",
        "model = MLP(\n",
        "    input_size=3072, hidden_size=1024, activation_fn=nn.ReLU(), device=device,\n",
        "    output_activation_fn=nn.Softmax(dim=0)\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "# instantiate MNIST train and validation datasets\n",
        "train_dataset = datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor()\n",
        ")\n",
        "val_dataset = datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor()\n",
        ")\n",
        "print(\"train dataset size = \", len(train_dataset))\n",
        "print(\"validation dataset size = \", len(val_dataset))\n",
        "\n",
        "# instantiate dataloaders\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=4\n",
        ")\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=4\n",
        ")\n",
        "num_train_batches = len(train_dataloader)\n",
        "num_val_batches = len(val_dataloader)\n",
        "\n",
        "epoch_loss = 0.0\n",
        "train_losses, val_losses = [], []\n",
        "train_predictions, val_predictions = [], []\n",
        "train_labels, val_labels = [], []\n",
        "train_accuracies, val_accuracies = [], []\n",
        "\n",
        "avg_grad_norms = []\n",
        "\n",
        "# DO NOT MODIFY LOSS FUNCTION BELOW\n",
        "##############################################################################\n",
        "loss_crt = nn.CrossEntropyLoss()\n",
        "##############################################################################\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
        "for epoch_idx in range(NUM_EPOCHS):\n",
        "    train_epoch_loss = 0.0\n",
        "    model.train()\n",
        "    for batch_images, batch_labels in train_dataloader:\n",
        "        batch_labels = batch_labels.to(device)\n",
        "\n",
        "        # feedforward\n",
        "        # batch_size x 10\n",
        "        out = model(batch_images)\n",
        "\n",
        "        batch_predictions = torch.argmax(out, dim=1)\n",
        "        train_predictions += batch_predictions.tolist()\n",
        "        train_labels += batch_labels.tolist()\n",
        "\n",
        "        # compute loss\n",
        "        loss = loss_crt(out, batch_labels)\n",
        "        loss.backward()\n",
        "\n",
        "        for name, param in model.named_parameters():\n",
        "            epoch_grad_norms = []\n",
        "            if param.grad is not None:\n",
        "                epoch_grad_norms.append(torch.norm(param.grad).cpu().numpy())\n",
        "            avg_grad_norms.append((sum(epoch_grad_norms) / len(epoch_grad_norms)))\n",
        "\n",
        "\n",
        "        optimizer.step()\n",
        "        train_epoch_loss += loss.item()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        val_epoch_loss = 0.0\n",
        "        for batch_images, batch_labels in val_dataloader:\n",
        "            batch_labels = batch_labels.to(device)\n",
        "\n",
        "            # batch_size x 10\n",
        "            # feedforward\n",
        "            out = model(batch_images)\n",
        "            batch_predictions = torch.argmax(out, dim=1)\n",
        "            val_predictions += batch_predictions.tolist()\n",
        "            val_labels += batch_labels.tolist()\n",
        "\n",
        "            # compute loss\n",
        "            loss = loss_crt(out, batch_labels)\n",
        "            val_epoch_loss += loss.item()\n",
        "\n",
        "    train_epoch_loss /= num_train_batches\n",
        "    val_epoch_loss /= num_val_batches\n",
        "    train_losses.append(train_epoch_loss)\n",
        "    val_losses.append(val_epoch_loss)\n",
        "\n",
        "    train_acc = accuracy_score(train_labels, train_predictions)\n",
        "    val_acc = accuracy_score(val_labels, val_predictions)\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_accuracies.append(val_acc)\n",
        "    print(\"epoch %d, train acc=%f, val acc=%f\" % (\n",
        "        epoch_idx,\n",
        "        train_acc,\n",
        "        val_acc\n",
        "    ))"
      ],
      "metadata": {
        "id": "3FLYyi8S9NOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "plt.plot(range(0,len(train_losses)), train_losses, 'g', label='Training loss')\n",
        "plt.plot(range(0,len(train_losses)), val_losses, 'b', label='Validation loss')\n",
        "plt.title('Training and Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.plot(range(0,len(train_accuracies)), train_accuracies, 'g', label='Training accuracy')\n",
        "plt.plot(range(0,len(train_accuracies)), val_accuracies, 'b', label='Validation accuracy')\n",
        "plt.title('Training and Validation accuracies')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.plot(range(0,len(avg_grad_norms)), avg_grad_norms, 'g')\n",
        "plt.title('Avg gradient norms')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Grad norm')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CBdSaezhEa6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 12: Let's set things in order..."
      ],
      "metadata": {
        "id": "xFPDb07LActS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_size: int,\n",
        "                 hidden_size: int,\n",
        "                 device: torch.device,\n",
        "                 activation_fn: Callable):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.hidden_layer = nn.Linear(input_size, hidden_size)\n",
        "        self.output_layer = nn.Linear(hidden_size, 3)\n",
        "        self.activation_fn = activation_fn\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, x):\n",
        "        # batch_size x 7\n",
        "        x = x.to(self.device)\n",
        "\n",
        "        h = self.hidden_layer(x)\n",
        "        h = self.activation_fn(h)\n",
        "        out = self.output_layer(h)\n",
        "\n",
        "        return out\n",
        "\n",
        "BATCH_SIZE=32\n",
        "\n",
        "# if you encounter a vague CUDA error message, move the operations to CPU then\n",
        "# run the code again. The error message is usually more helpful.\n",
        "#device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "device = torch.device('cpu')\n",
        "print(\"device = \", device)\n",
        "\n",
        "# instantiate model\n",
        "model = MLP(\n",
        "    input_size=7, hidden_size=128, activation_fn=nn.ReLU(), device=device\n",
        ")\n",
        "\n",
        "# move model to GPU (Module.to() is an in-place operation)\n",
        "model.to(device)\n",
        "\n",
        "# download Wheat Seeds dataset\n",
        "!wget --no-check-certificate \\\n",
        "https://raw.githubusercontent.com/jbrownlee/Datasets/master/wheat-seeds.csv \\\n",
        "-O /tmp/wheat.csv\n",
        "\n",
        "# read Wheat Seeds dataset from csv\n",
        "# Dataset has 209 examples. Each example has 7 attributes (features).\n",
        "# It's a classification task with 3 classes (1, 2 and 3)\n",
        "data = pd.read_csv(\"/tmp/wheat.csv\")\n",
        "\n",
        "# solution: subtract 1 from the `labels` tensor\n",
        "x = torch.tensor(data.values, dtype=torch.float32)\n",
        "data, labels = x[:,:-1], x[:,-1].long()-1\n",
        "\n",
        "# we create a TensorDataset, which is a type of Dataset that wraps Tensors.\n",
        "# https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset\n",
        "# Examples are indexed over the first dimension, so the first dimension of\n",
        "# the Tensors must be the same (209 in our case)\n",
        "\n",
        "train_percent = 0.75\n",
        "num_samples = len(data)\n",
        "train_indices = range(0, int(num_samples * train_percent))\n",
        "valid_indices = range(int(num_samples * train_percent), len(data))\n",
        "\n",
        "\n",
        "training_dataset = TensorDataset(data[train_indices], labels[train_indices])\n",
        "validation_dataset = TensorDataset(data[valid_indices], labels[valid_indices])\n",
        "\n",
        "# instantiate train dataloader\n",
        "train_dataloader = DataLoader(\n",
        "    training_dataset,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "# instantiate validation dataloader\n",
        "val_dataloader = DataLoader(\n",
        "    validation_dataset,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "loss_crt = nn.CrossEntropyLoss()\n",
        "num_train_batches = len(train_dataloader)\n",
        "num_val_batches = len(val_dataloader)\n",
        "\n",
        "epoch_loss = 0.0\n",
        "train_losses, val_losses = [], []\n",
        "train_predictions, val_predictions = [], []\n",
        "train_labels, val_labels = [], []\n",
        "train_accuracies, val_accuracies = [], []\n",
        "\n",
        "loss_crt = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
        "NUM_EPOCHS = 100\n",
        "\n",
        "for epoch_idx in range(NUM_EPOCHS):\n",
        "    train_labels = []\n",
        "    val_labels = []\n",
        "    train_preds = []\n",
        "    val_preds = []\n",
        "\n",
        "    train_epoch_loss = 0.0\n",
        "    model.train()\n",
        "    for batch_images, batch_labels in train_dataloader:\n",
        "        model.zero_grad()\n",
        "        batch_labels = batch_labels.to(device)\n",
        "\n",
        "        # feedforward\n",
        "        out = model(batch_images)\n",
        "\n",
        "        batch_predictions = torch.argmax(out, dim=1)\n",
        "\n",
        "        # compute loss\n",
        "        loss = loss_crt(out, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_epoch_loss += loss.item()\n",
        "        train_labels += batch_labels.tolist()\n",
        "        train_preds += batch_predictions.tolist()\n",
        "\n",
        "    val_preds = []\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        val_epoch_loss = 0.0\n",
        "        for batch_images, batch_labels in val_dataloader:\n",
        "            batch_labels = batch_labels.to(device)\n",
        "\n",
        "            # batch_size x 10\n",
        "            # feedforward\n",
        "            out = model(batch_images)\n",
        "            batch_predictions = torch.argmax(out, dim=1)\n",
        "\n",
        "            # compute loss\n",
        "            loss = loss_crt(out, batch_labels)\n",
        "            val_epoch_loss += loss.item()\n",
        "            val_labels += batch_labels.tolist()\n",
        "            val_preds += batch_predictions.tolist()\n",
        "\n",
        "    train_epoch_loss /= num_train_batches\n",
        "    val_epoch_loss /= num_val_batches\n",
        "    train_acc = accuracy_score(train_labels, train_preds)\n",
        "    valid_acc = accuracy_score(val_labels, val_preds)\n",
        "\n",
        "    print(\"epoch %d, train loss=%f, val loss=%f, train acc=%f, val acc=%f\" % (\n",
        "        epoch_idx, train_epoch_loss, val_epoch_loss, train_acc, valid_acc\n",
        "    ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AP46TnWFAdAg",
        "outputId": "c0162f76-3eec-443b-c418-d3b5c0b4f386"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device =  cpu\n",
            "--2024-03-19 14:00:43--  https://raw.githubusercontent.com/jbrownlee/Datasets/master/wheat-seeds.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9301 (9.1K) [text/plain]\n",
            "Saving to: ‘/tmp/wheat.csv’\n",
            "\n",
            "/tmp/wheat.csv      100%[===================>]   9.08K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2024-03-19 14:00:44 (16.2 MB/s) - ‘/tmp/wheat.csv’ saved [9301/9301]\n",
            "\n",
            "epoch 0, train loss=1.797644, val loss=2.491622, train acc=0.448718, val acc=0.000000\n",
            "epoch 1, train loss=1.527023, val loss=2.227722, train acc=0.448718, val acc=0.000000\n",
            "epoch 2, train loss=1.332701, val loss=2.019177, train acc=0.448718, val acc=0.000000\n",
            "epoch 3, train loss=1.193527, val loss=1.880535, train acc=0.448718, val acc=0.000000\n",
            "epoch 4, train loss=1.105950, val loss=1.806260, train acc=0.448718, val acc=0.000000\n",
            "epoch 5, train loss=1.057975, val loss=1.779538, train acc=0.442308, val acc=0.000000\n",
            "epoch 6, train loss=1.033495, val loss=1.780981, train acc=0.365385, val acc=0.000000\n",
            "epoch 7, train loss=1.019322, val loss=1.794997, train acc=0.320513, val acc=0.000000\n",
            "epoch 8, train loss=1.008138, val loss=1.811842, train acc=0.307692, val acc=0.000000\n",
            "epoch 9, train loss=0.997645, val loss=1.826615, train acc=0.314103, val acc=0.000000\n",
            "epoch 10, train loss=0.987990, val loss=1.837232, train acc=0.333333, val acc=0.000000\n",
            "epoch 11, train loss=0.979898, val loss=1.843287, train acc=0.358974, val acc=0.000000\n",
            "epoch 12, train loss=0.973544, val loss=1.845031, train acc=0.378205, val acc=0.000000\n",
            "epoch 13, train loss=0.968687, val loss=1.842833, train acc=0.391026, val acc=0.000000\n",
            "epoch 14, train loss=0.964673, val loss=1.837263, train acc=0.410256, val acc=0.000000\n",
            "epoch 15, train loss=0.961002, val loss=1.829193, train acc=0.423077, val acc=0.000000\n",
            "epoch 16, train loss=0.957442, val loss=1.819484, train acc=0.416667, val acc=0.000000\n",
            "epoch 17, train loss=0.953867, val loss=1.808715, train acc=0.416667, val acc=0.000000\n",
            "epoch 18, train loss=0.950151, val loss=1.797388, train acc=0.423077, val acc=0.000000\n",
            "epoch 19, train loss=0.946365, val loss=1.785894, train acc=0.416667, val acc=0.000000\n",
            "epoch 20, train loss=0.942531, val loss=1.774608, train acc=0.423077, val acc=0.000000\n",
            "epoch 21, train loss=0.938653, val loss=1.763655, train acc=0.429487, val acc=0.000000\n",
            "epoch 22, train loss=0.934780, val loss=1.753076, train acc=0.429487, val acc=0.000000\n",
            "epoch 23, train loss=0.930942, val loss=1.742866, train acc=0.435897, val acc=0.000000\n",
            "epoch 24, train loss=0.927154, val loss=1.733141, train acc=0.435897, val acc=0.000000\n",
            "epoch 25, train loss=0.923358, val loss=1.723806, train acc=0.442308, val acc=0.000000\n",
            "epoch 26, train loss=0.919445, val loss=1.714804, train acc=0.442308, val acc=0.000000\n",
            "epoch 27, train loss=0.915365, val loss=1.706052, train acc=0.448718, val acc=0.000000\n",
            "epoch 28, train loss=0.911027, val loss=1.697647, train acc=0.455128, val acc=0.000000\n",
            "epoch 29, train loss=0.906583, val loss=1.689449, train acc=0.461538, val acc=0.000000\n",
            "epoch 30, train loss=0.902227, val loss=1.681399, train acc=0.461538, val acc=0.000000\n",
            "epoch 31, train loss=0.898048, val loss=1.673442, train acc=0.467949, val acc=0.000000\n",
            "epoch 32, train loss=0.894063, val loss=1.665360, train acc=0.467949, val acc=0.000000\n",
            "epoch 33, train loss=0.890213, val loss=1.657107, train acc=0.474359, val acc=0.000000\n",
            "epoch 34, train loss=0.886429, val loss=1.648809, train acc=0.480769, val acc=0.000000\n",
            "epoch 35, train loss=0.882732, val loss=1.640557, train acc=0.474359, val acc=0.000000\n",
            "epoch 36, train loss=0.879072, val loss=1.632455, train acc=0.480769, val acc=0.000000\n",
            "epoch 37, train loss=0.875446, val loss=1.624619, train acc=0.480769, val acc=0.000000\n",
            "epoch 38, train loss=0.871859, val loss=1.617011, train acc=0.500000, val acc=0.000000\n",
            "epoch 39, train loss=0.868312, val loss=1.609539, train acc=0.500000, val acc=0.000000\n",
            "epoch 40, train loss=0.864825, val loss=1.602140, train acc=0.506410, val acc=0.000000\n",
            "epoch 41, train loss=0.861386, val loss=1.594764, train acc=0.506410, val acc=0.000000\n",
            "epoch 42, train loss=0.857991, val loss=1.587365, train acc=0.506410, val acc=0.000000\n",
            "epoch 43, train loss=0.854627, val loss=1.579967, train acc=0.506410, val acc=0.000000\n",
            "epoch 44, train loss=0.851301, val loss=1.572637, train acc=0.506410, val acc=0.000000\n",
            "epoch 45, train loss=0.848014, val loss=1.565416, train acc=0.506410, val acc=0.000000\n",
            "epoch 46, train loss=0.844748, val loss=1.558325, train acc=0.519231, val acc=0.000000\n",
            "epoch 47, train loss=0.841519, val loss=1.551414, train acc=0.519231, val acc=0.000000\n",
            "epoch 48, train loss=0.838320, val loss=1.544646, train acc=0.519231, val acc=0.000000\n",
            "epoch 49, train loss=0.835158, val loss=1.537986, train acc=0.525641, val acc=0.000000\n",
            "epoch 50, train loss=0.832024, val loss=1.531393, train acc=0.532051, val acc=0.000000\n",
            "epoch 51, train loss=0.828898, val loss=1.524875, train acc=0.532051, val acc=0.000000\n",
            "epoch 52, train loss=0.825786, val loss=1.518436, train acc=0.532051, val acc=0.000000\n",
            "epoch 53, train loss=0.822675, val loss=1.512095, train acc=0.532051, val acc=0.000000\n",
            "epoch 54, train loss=0.819583, val loss=1.505841, train acc=0.538462, val acc=0.000000\n",
            "epoch 55, train loss=0.816504, val loss=1.499693, train acc=0.557692, val acc=0.000000\n",
            "epoch 56, train loss=0.813446, val loss=1.493619, train acc=0.551282, val acc=0.000000\n",
            "epoch 57, train loss=0.810402, val loss=1.487597, train acc=0.557692, val acc=0.000000\n",
            "epoch 58, train loss=0.807372, val loss=1.481579, train acc=0.570513, val acc=0.000000\n",
            "epoch 59, train loss=0.804374, val loss=1.475588, train acc=0.570513, val acc=0.000000\n",
            "epoch 60, train loss=0.801404, val loss=1.469625, train acc=0.589744, val acc=0.000000\n",
            "epoch 61, train loss=0.798465, val loss=1.463673, train acc=0.596154, val acc=0.000000\n",
            "epoch 62, train loss=0.795565, val loss=1.457754, train acc=0.602564, val acc=0.000000\n",
            "epoch 63, train loss=0.792681, val loss=1.451886, train acc=0.615385, val acc=0.000000\n",
            "epoch 64, train loss=0.789832, val loss=1.446109, train acc=0.621795, val acc=0.018868\n",
            "epoch 65, train loss=0.787010, val loss=1.440340, train acc=0.621795, val acc=0.018868\n",
            "epoch 66, train loss=0.784189, val loss=1.434460, train acc=0.634615, val acc=0.018868\n",
            "epoch 67, train loss=0.781394, val loss=1.428544, train acc=0.634615, val acc=0.056604\n",
            "epoch 68, train loss=0.778617, val loss=1.422634, train acc=0.628205, val acc=0.056604\n",
            "epoch 69, train loss=0.775860, val loss=1.416773, train acc=0.628205, val acc=0.056604\n",
            "epoch 70, train loss=0.773121, val loss=1.410975, train acc=0.647436, val acc=0.056604\n",
            "epoch 71, train loss=0.770404, val loss=1.405210, train acc=0.641026, val acc=0.056604\n",
            "epoch 72, train loss=0.767715, val loss=1.399453, train acc=0.641026, val acc=0.056604\n",
            "epoch 73, train loss=0.765050, val loss=1.393731, train acc=0.641026, val acc=0.056604\n",
            "epoch 74, train loss=0.762410, val loss=1.388051, train acc=0.647436, val acc=0.056604\n",
            "epoch 75, train loss=0.759789, val loss=1.382415, train acc=0.660256, val acc=0.056604\n",
            "epoch 76, train loss=0.757188, val loss=1.376831, train acc=0.660256, val acc=0.075472\n",
            "epoch 77, train loss=0.754608, val loss=1.371291, train acc=0.666667, val acc=0.075472\n",
            "epoch 78, train loss=0.752053, val loss=1.365790, train acc=0.673077, val acc=0.094340\n",
            "epoch 79, train loss=0.749523, val loss=1.360324, train acc=0.673077, val acc=0.094340\n",
            "epoch 80, train loss=0.746998, val loss=1.354895, train acc=0.679487, val acc=0.094340\n",
            "epoch 81, train loss=0.744496, val loss=1.349509, train acc=0.692308, val acc=0.094340\n",
            "epoch 82, train loss=0.741999, val loss=1.344175, train acc=0.692308, val acc=0.094340\n",
            "epoch 83, train loss=0.739518, val loss=1.338921, train acc=0.692308, val acc=0.113208\n",
            "epoch 84, train loss=0.737049, val loss=1.333723, train acc=0.698718, val acc=0.113208\n",
            "epoch 85, train loss=0.734601, val loss=1.328563, train acc=0.698718, val acc=0.113208\n",
            "epoch 86, train loss=0.732181, val loss=1.323423, train acc=0.698718, val acc=0.113208\n",
            "epoch 87, train loss=0.729775, val loss=1.318296, train acc=0.705128, val acc=0.113208\n",
            "epoch 88, train loss=0.727392, val loss=1.313186, train acc=0.705128, val acc=0.113208\n",
            "epoch 89, train loss=0.725029, val loss=1.308100, train acc=0.711538, val acc=0.113208\n",
            "epoch 90, train loss=0.722687, val loss=1.303057, train acc=0.724359, val acc=0.113208\n",
            "epoch 91, train loss=0.720363, val loss=1.298059, train acc=0.724359, val acc=0.113208\n",
            "epoch 92, train loss=0.718062, val loss=1.293127, train acc=0.724359, val acc=0.113208\n",
            "epoch 93, train loss=0.715778, val loss=1.288229, train acc=0.724359, val acc=0.113208\n",
            "epoch 94, train loss=0.713507, val loss=1.283358, train acc=0.730769, val acc=0.113208\n",
            "epoch 95, train loss=0.711257, val loss=1.278539, train acc=0.730769, val acc=0.113208\n",
            "epoch 96, train loss=0.709020, val loss=1.273757, train acc=0.730769, val acc=0.113208\n",
            "epoch 97, train loss=0.706794, val loss=1.269006, train acc=0.737179, val acc=0.113208\n",
            "epoch 98, train loss=0.704580, val loss=1.264283, train acc=0.750000, val acc=0.132075\n",
            "epoch 99, train loss=0.702383, val loss=1.259569, train acc=0.756410, val acc=0.132075\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 13: Weighing the evidence\n"
      ],
      "metadata": {
        "id": "R8ZKdrPnAqaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_size: int,\n",
        "                 hidden_size: int,\n",
        "                 device: torch.device,\n",
        "                 activation_fn: Callable):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.hidden_layer = nn.Linear(input_size, hidden_size)\n",
        "        self.output_layer = nn.Linear(hidden_size, 3)\n",
        "        self.activation_fn = activation_fn\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, x):\n",
        "        # batch_size x 7\n",
        "        x = x.to(self.device)\n",
        "\n",
        "        h = self.hidden_layer(x)\n",
        "        h = self.activation_fn(h)\n",
        "        out = self.output_layer(h)\n",
        "\n",
        "        return out\n",
        "\n",
        "BATCH_SIZE=32\n",
        "\n",
        "# if you encounter a vague CUDA error message, move the operations to CPU then\n",
        "# run the code again. The error message is usually more helpful.\n",
        "#device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "device = torch.device('cpu')\n",
        "print(\"device = \", device)\n",
        "\n",
        "# instantiate model\n",
        "model = MLP(\n",
        "    input_size=7, hidden_size=128, activation_fn=nn.ReLU(), device=device\n",
        ")\n",
        "\n",
        "# move model to GPU (Module.to() is an in-place operation)\n",
        "model.to(device)\n",
        "\n",
        "# download Wheat Seeds dataset\n",
        "!wget --no-check-certificate \\\n",
        "https://raw.githubusercontent.com/jbrownlee/Datasets/master/wheat-seeds.csv \\\n",
        "-O /tmp/wheat.csv\n",
        "\n",
        "# read Wheat Seeds dataset from csv\n",
        "# Dataset has 209 examples. Each example has 7 attributes (features).\n",
        "# It's a classification task with 3 classes (1, 2 and 3)\n",
        "data = pd.read_csv(\"/tmp/wheat.csv\")\n",
        "\n",
        "# solution: subtract 1 from the `labels` tensor\n",
        "x = torch.tensor(data.values, dtype=torch.float32)\n",
        "data, labels = x[:,:-1], x[:,-1].long()-1\n",
        "\n",
        "# we create a TensorDataset, which is a type of Dataset that wraps Tensors.\n",
        "# https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset\n",
        "# Examples are indexed over the first dimension, so the first dimension of\n",
        "# the Tensors must be the same (209 in our case)\n",
        "\n",
        "train_percent = 0.75\n",
        "num_samples = len(data)\n",
        "train_indices = range(0, int(num_samples * train_percent))\n",
        "valid_indices = range(int(num_samples * train_percent), len(data))\n",
        "\n",
        "class_counts = {\n",
        "    0: 15,\n",
        "    1: 65,\n",
        "    2: 15\n",
        "}\n",
        "\n",
        "train_indices = []\n",
        "for cls in class_counts.keys():\n",
        "    cls_indices = (labels == cls).nonzero().reshape((-1)).tolist()\n",
        "    train_indices += cls_indices[:class_counts[cls]]\n",
        "\n",
        "valid_indices = list(set(range(num_samples)) - set(train_indices))\n",
        "\n",
        "training_dataset = TensorDataset(data[train_indices], labels[train_indices])\n",
        "validation_dataset = TensorDataset(data[valid_indices], labels[valid_indices])\n",
        "\n",
        "# instantiate train dataloader\n",
        "train_dataloader = DataLoader(\n",
        "    training_dataset,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "# instantiate validation dataloader\n",
        "val_dataloader = DataLoader(\n",
        "    validation_dataset,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "loss_crt = nn.CrossEntropyLoss()\n",
        "num_train_batches = len(train_dataloader)\n",
        "num_val_batches = len(val_dataloader)\n",
        "\n",
        "epoch_loss = 0.0\n",
        "train_losses, val_losses = [], []\n",
        "train_predictions, val_predictions = [], []\n",
        "train_labels, val_labels = [], []\n",
        "train_accuracies, val_accuracies = [], []\n",
        "\n",
        "loss_crt = nn.CrossEntropyLoss(weight=torch.Tensor(\n",
        "      [count / len(train_indices) for (cls, count) in class_counts.items()]\n",
        "    ))\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
        "NUM_EPOCHS = 300\n",
        "\n",
        "for epoch_idx in range(NUM_EPOCHS):\n",
        "    train_labels = []\n",
        "    val_labels = []\n",
        "    train_preds = []\n",
        "    val_preds = []\n",
        "\n",
        "    train_epoch_loss = 0.0\n",
        "    model.train()\n",
        "    for batch_images, batch_labels in train_dataloader:\n",
        "        model.zero_grad()\n",
        "        batch_labels = batch_labels.to(device)\n",
        "\n",
        "        # feedforward\n",
        "        out = model(batch_images)\n",
        "\n",
        "        batch_predictions = torch.argmax(out, dim=1)\n",
        "\n",
        "        # compute loss\n",
        "        loss = loss_crt(out, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_epoch_loss += loss.item()\n",
        "        train_labels += batch_labels.tolist()\n",
        "        train_preds += batch_predictions.tolist()\n",
        "\n",
        "    val_preds = []\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        val_epoch_loss = 0.0\n",
        "        for batch_images, batch_labels in val_dataloader:\n",
        "            batch_labels = batch_labels.to(device)\n",
        "\n",
        "            # batch_size x 10\n",
        "            # feedforward\n",
        "            out = model(batch_images)\n",
        "            batch_predictions = torch.argmax(out, dim=1)\n",
        "\n",
        "            # compute loss\n",
        "            loss = loss_crt(out, batch_labels)\n",
        "            val_epoch_loss += loss.item()\n",
        "            val_labels += batch_labels.tolist()\n",
        "            val_preds += batch_predictions.tolist()\n",
        "\n",
        "    train_epoch_loss /= num_train_batches\n",
        "    val_epoch_loss /= num_val_batches\n",
        "    train_acc = accuracy_score(train_labels, train_preds)\n",
        "    valid_acc = accuracy_score(val_labels, val_preds)\n",
        "\n",
        "    print(\"epoch %d, train loss=%f, val loss=%f, train acc=%f, val acc=%f\" % (\n",
        "        epoch_idx, train_epoch_loss, val_epoch_loss, train_acc, valid_acc\n",
        "    ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hczSwdTFAty6",
        "outputId": "6f8166ea-1e79-46be-a921-562234028ce8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device =  cpu\n",
            "--2024-03-19 14:00:50--  https://raw.githubusercontent.com/jbrownlee/Datasets/master/wheat-seeds.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9301 (9.1K) [text/plain]\n",
            "Saving to: ‘/tmp/wheat.csv’\n",
            "\n",
            "\r/tmp/wheat.csv        0%[                    ]       0  --.-KB/s               \r/tmp/wheat.csv      100%[===================>]   9.08K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-03-19 14:00:50 (126 MB/s) - ‘/tmp/wheat.csv’ saved [9301/9301]\n",
            "\n",
            "epoch 0, train loss=5.201918, val loss=1.497913, train acc=0.157895, val acc=0.482456\n",
            "epoch 1, train loss=4.565321, val loss=1.334123, train acc=0.157895, val acc=0.482456\n",
            "epoch 2, train loss=3.947047, val loss=1.207482, train acc=0.157895, val acc=0.482456\n",
            "epoch 3, train loss=3.347959, val loss=1.118400, train acc=0.157895, val acc=0.482456\n",
            "epoch 4, train loss=2.771556, val loss=1.066557, train acc=0.157895, val acc=0.482456\n",
            "epoch 5, train loss=2.225700, val loss=1.053741, train acc=0.157895, val acc=0.482456\n",
            "epoch 6, train loss=1.725223, val loss=1.084880, train acc=0.157895, val acc=0.482456\n",
            "epoch 7, train loss=1.291840, val loss=1.165548, train acc=0.157895, val acc=0.482456\n",
            "epoch 8, train loss=0.947934, val loss=1.295195, train acc=0.168421, val acc=0.043860\n",
            "epoch 9, train loss=0.704156, val loss=1.462187, train acc=0.684211, val acc=0.043860\n",
            "epoch 10, train loss=0.550852, val loss=1.646360, train acc=0.684211, val acc=0.043860\n",
            "epoch 11, train loss=0.464427, val loss=1.826879, train acc=0.684211, val acc=0.043860\n",
            "epoch 12, train loss=0.420003, val loss=1.988451, train acc=0.684211, val acc=0.043860\n",
            "epoch 13, train loss=0.399070, val loss=2.123202, train acc=0.684211, val acc=0.043860\n",
            "epoch 14, train loss=0.390140, val loss=2.228932, train acc=0.684211, val acc=0.043860\n",
            "epoch 15, train loss=0.386812, val loss=2.306995, train acc=0.684211, val acc=0.043860\n",
            "epoch 16, train loss=0.385701, val loss=2.360579, train acc=0.684211, val acc=0.043860\n",
            "epoch 17, train loss=0.385201, val loss=2.393337, train acc=0.684211, val acc=0.043860\n",
            "epoch 18, train loss=0.384613, val loss=2.409259, train acc=0.684211, val acc=0.043860\n",
            "epoch 19, train loss=0.383698, val loss=2.411885, train acc=0.684211, val acc=0.043860\n",
            "epoch 20, train loss=0.382408, val loss=2.404476, train acc=0.684211, val acc=0.043860\n",
            "epoch 21, train loss=0.380816, val loss=2.389709, train acc=0.684211, val acc=0.043860\n",
            "epoch 22, train loss=0.379083, val loss=2.369934, train acc=0.684211, val acc=0.043860\n",
            "epoch 23, train loss=0.377355, val loss=2.347282, train acc=0.684211, val acc=0.043860\n",
            "epoch 24, train loss=0.375720, val loss=2.323327, train acc=0.684211, val acc=0.043860\n",
            "epoch 25, train loss=0.374222, val loss=2.299308, train acc=0.684211, val acc=0.043860\n",
            "epoch 26, train loss=0.372875, val loss=2.276156, train acc=0.684211, val acc=0.043860\n",
            "epoch 27, train loss=0.371688, val loss=2.254487, train acc=0.684211, val acc=0.043860\n",
            "epoch 28, train loss=0.370643, val loss=2.234744, train acc=0.684211, val acc=0.043860\n",
            "epoch 29, train loss=0.369705, val loss=2.217144, train acc=0.684211, val acc=0.043860\n",
            "epoch 30, train loss=0.368844, val loss=2.201699, train acc=0.684211, val acc=0.043860\n",
            "epoch 31, train loss=0.368036, val loss=2.188356, train acc=0.684211, val acc=0.043860\n",
            "epoch 32, train loss=0.367259, val loss=2.176938, train acc=0.684211, val acc=0.043860\n",
            "epoch 33, train loss=0.366511, val loss=2.167293, train acc=0.684211, val acc=0.043860\n",
            "epoch 34, train loss=0.365783, val loss=2.159171, train acc=0.684211, val acc=0.043860\n",
            "epoch 35, train loss=0.365072, val loss=2.152270, train acc=0.684211, val acc=0.043860\n",
            "epoch 36, train loss=0.364373, val loss=2.146302, train acc=0.684211, val acc=0.043860\n",
            "epoch 37, train loss=0.363683, val loss=2.140969, train acc=0.684211, val acc=0.043860\n",
            "epoch 38, train loss=0.363002, val loss=2.135985, train acc=0.684211, val acc=0.043860\n",
            "epoch 39, train loss=0.362338, val loss=2.131239, train acc=0.684211, val acc=0.043860\n",
            "epoch 40, train loss=0.361687, val loss=2.126628, train acc=0.684211, val acc=0.043860\n",
            "epoch 41, train loss=0.361046, val loss=2.122045, train acc=0.684211, val acc=0.043860\n",
            "epoch 42, train loss=0.360411, val loss=2.117407, train acc=0.684211, val acc=0.043860\n",
            "epoch 43, train loss=0.359779, val loss=2.112681, train acc=0.684211, val acc=0.043860\n",
            "epoch 44, train loss=0.359147, val loss=2.107859, train acc=0.684211, val acc=0.043860\n",
            "epoch 45, train loss=0.358513, val loss=2.102925, train acc=0.684211, val acc=0.043860\n",
            "epoch 46, train loss=0.357876, val loss=2.097915, train acc=0.684211, val acc=0.043860\n",
            "epoch 47, train loss=0.357236, val loss=2.092832, train acc=0.684211, val acc=0.043860\n",
            "epoch 48, train loss=0.356590, val loss=2.087685, train acc=0.684211, val acc=0.043860\n",
            "epoch 49, train loss=0.355941, val loss=2.082502, train acc=0.684211, val acc=0.043860\n",
            "epoch 50, train loss=0.355289, val loss=2.077308, train acc=0.684211, val acc=0.043860\n",
            "epoch 51, train loss=0.354633, val loss=2.072107, train acc=0.684211, val acc=0.043860\n",
            "epoch 52, train loss=0.353973, val loss=2.066933, train acc=0.684211, val acc=0.043860\n",
            "epoch 53, train loss=0.353310, val loss=2.061788, train acc=0.684211, val acc=0.043860\n",
            "epoch 54, train loss=0.352644, val loss=2.056680, train acc=0.684211, val acc=0.043860\n",
            "epoch 55, train loss=0.351976, val loss=2.051633, train acc=0.684211, val acc=0.043860\n",
            "epoch 56, train loss=0.351308, val loss=2.046655, train acc=0.684211, val acc=0.043860\n",
            "epoch 57, train loss=0.350640, val loss=2.041736, train acc=0.684211, val acc=0.043860\n",
            "epoch 58, train loss=0.349967, val loss=2.036889, train acc=0.684211, val acc=0.043860\n",
            "epoch 59, train loss=0.349292, val loss=2.032093, train acc=0.684211, val acc=0.043860\n",
            "epoch 60, train loss=0.348612, val loss=2.027341, train acc=0.684211, val acc=0.043860\n",
            "epoch 61, train loss=0.347928, val loss=2.022605, train acc=0.684211, val acc=0.043860\n",
            "epoch 62, train loss=0.347241, val loss=2.017880, train acc=0.684211, val acc=0.043860\n",
            "epoch 63, train loss=0.346551, val loss=2.013158, train acc=0.684211, val acc=0.043860\n",
            "epoch 64, train loss=0.345859, val loss=2.008442, train acc=0.684211, val acc=0.043860\n",
            "epoch 65, train loss=0.345166, val loss=2.003770, train acc=0.684211, val acc=0.043860\n",
            "epoch 66, train loss=0.344472, val loss=1.999152, train acc=0.684211, val acc=0.043860\n",
            "epoch 67, train loss=0.343778, val loss=1.994540, train acc=0.684211, val acc=0.043860\n",
            "epoch 68, train loss=0.343088, val loss=1.989948, train acc=0.684211, val acc=0.043860\n",
            "epoch 69, train loss=0.342402, val loss=1.985365, train acc=0.684211, val acc=0.043860\n",
            "epoch 70, train loss=0.341717, val loss=1.980800, train acc=0.684211, val acc=0.043860\n",
            "epoch 71, train loss=0.341033, val loss=1.976241, train acc=0.684211, val acc=0.043860\n",
            "epoch 72, train loss=0.340348, val loss=1.971682, train acc=0.684211, val acc=0.043860\n",
            "epoch 73, train loss=0.339663, val loss=1.967143, train acc=0.684211, val acc=0.043860\n",
            "epoch 74, train loss=0.338976, val loss=1.962609, train acc=0.684211, val acc=0.043860\n",
            "epoch 75, train loss=0.338289, val loss=1.958062, train acc=0.684211, val acc=0.043860\n",
            "epoch 76, train loss=0.337601, val loss=1.953494, train acc=0.684211, val acc=0.043860\n",
            "epoch 77, train loss=0.336912, val loss=1.948894, train acc=0.684211, val acc=0.043860\n",
            "epoch 78, train loss=0.336228, val loss=1.944250, train acc=0.684211, val acc=0.043860\n",
            "epoch 79, train loss=0.335545, val loss=1.939567, train acc=0.684211, val acc=0.043860\n",
            "epoch 80, train loss=0.334866, val loss=1.934848, train acc=0.684211, val acc=0.043860\n",
            "epoch 81, train loss=0.334188, val loss=1.930128, train acc=0.684211, val acc=0.043860\n",
            "epoch 82, train loss=0.333512, val loss=1.925443, train acc=0.684211, val acc=0.043860\n",
            "epoch 83, train loss=0.332837, val loss=1.920770, train acc=0.684211, val acc=0.043860\n",
            "epoch 84, train loss=0.332168, val loss=1.916120, train acc=0.684211, val acc=0.043860\n",
            "epoch 85, train loss=0.331499, val loss=1.911469, train acc=0.684211, val acc=0.043860\n",
            "epoch 86, train loss=0.330832, val loss=1.906809, train acc=0.684211, val acc=0.043860\n",
            "epoch 87, train loss=0.330170, val loss=1.902132, train acc=0.684211, val acc=0.043860\n",
            "epoch 88, train loss=0.329509, val loss=1.897457, train acc=0.684211, val acc=0.043860\n",
            "epoch 89, train loss=0.328849, val loss=1.892804, train acc=0.684211, val acc=0.043860\n",
            "epoch 90, train loss=0.328190, val loss=1.888191, train acc=0.684211, val acc=0.043860\n",
            "epoch 91, train loss=0.327532, val loss=1.883617, train acc=0.684211, val acc=0.043860\n",
            "epoch 92, train loss=0.326875, val loss=1.879069, train acc=0.684211, val acc=0.043860\n",
            "epoch 93, train loss=0.326218, val loss=1.874538, train acc=0.684211, val acc=0.043860\n",
            "epoch 94, train loss=0.325562, val loss=1.870036, train acc=0.684211, val acc=0.043860\n",
            "epoch 95, train loss=0.324905, val loss=1.865550, train acc=0.684211, val acc=0.043860\n",
            "epoch 96, train loss=0.324248, val loss=1.861086, train acc=0.684211, val acc=0.043860\n",
            "epoch 97, train loss=0.323591, val loss=1.856638, train acc=0.684211, val acc=0.043860\n",
            "epoch 98, train loss=0.322934, val loss=1.852217, train acc=0.684211, val acc=0.043860\n",
            "epoch 99, train loss=0.322278, val loss=1.847828, train acc=0.684211, val acc=0.043860\n",
            "epoch 100, train loss=0.321628, val loss=1.843437, train acc=0.684211, val acc=0.043860\n",
            "epoch 101, train loss=0.320980, val loss=1.839065, train acc=0.684211, val acc=0.043860\n",
            "epoch 102, train loss=0.320331, val loss=1.834703, train acc=0.684211, val acc=0.043860\n",
            "epoch 103, train loss=0.319687, val loss=1.830331, train acc=0.684211, val acc=0.043860\n",
            "epoch 104, train loss=0.319050, val loss=1.825910, train acc=0.684211, val acc=0.043860\n",
            "epoch 105, train loss=0.318413, val loss=1.821443, train acc=0.684211, val acc=0.043860\n",
            "epoch 106, train loss=0.317776, val loss=1.816955, train acc=0.684211, val acc=0.043860\n",
            "epoch 107, train loss=0.317139, val loss=1.812461, train acc=0.684211, val acc=0.043860\n",
            "epoch 108, train loss=0.316499, val loss=1.807981, train acc=0.684211, val acc=0.043860\n",
            "epoch 109, train loss=0.315858, val loss=1.803515, train acc=0.684211, val acc=0.043860\n",
            "epoch 110, train loss=0.315216, val loss=1.799064, train acc=0.684211, val acc=0.043860\n",
            "epoch 111, train loss=0.314572, val loss=1.794633, train acc=0.684211, val acc=0.043860\n",
            "epoch 112, train loss=0.313930, val loss=1.790217, train acc=0.684211, val acc=0.043860\n",
            "epoch 113, train loss=0.313281, val loss=1.785831, train acc=0.684211, val acc=0.043860\n",
            "epoch 114, train loss=0.312624, val loss=1.781477, train acc=0.684211, val acc=0.043860\n",
            "epoch 115, train loss=0.311958, val loss=1.777136, train acc=0.684211, val acc=0.043860\n",
            "epoch 116, train loss=0.311277, val loss=1.772768, train acc=0.684211, val acc=0.043860\n",
            "epoch 117, train loss=0.310587, val loss=1.768385, train acc=0.684211, val acc=0.043860\n",
            "epoch 118, train loss=0.309895, val loss=1.763974, train acc=0.684211, val acc=0.043860\n",
            "epoch 119, train loss=0.309190, val loss=1.759543, train acc=0.684211, val acc=0.043860\n",
            "epoch 120, train loss=0.308475, val loss=1.755106, train acc=0.684211, val acc=0.043860\n",
            "epoch 121, train loss=0.307742, val loss=1.750602, train acc=0.684211, val acc=0.043860\n",
            "epoch 122, train loss=0.306988, val loss=1.746083, train acc=0.684211, val acc=0.043860\n",
            "epoch 123, train loss=0.306212, val loss=1.741513, train acc=0.684211, val acc=0.043860\n",
            "epoch 124, train loss=0.305426, val loss=1.736915, train acc=0.684211, val acc=0.043860\n",
            "epoch 125, train loss=0.304653, val loss=1.732275, train acc=0.684211, val acc=0.043860\n",
            "epoch 126, train loss=0.303940, val loss=1.727461, train acc=0.684211, val acc=0.043860\n",
            "epoch 127, train loss=0.303254, val loss=1.722596, train acc=0.684211, val acc=0.043860\n",
            "epoch 128, train loss=0.302577, val loss=1.717706, train acc=0.684211, val acc=0.043860\n",
            "epoch 129, train loss=0.301909, val loss=1.712737, train acc=0.684211, val acc=0.043860\n",
            "epoch 130, train loss=0.301247, val loss=1.707764, train acc=0.684211, val acc=0.043860\n",
            "epoch 131, train loss=0.300584, val loss=1.702839, train acc=0.684211, val acc=0.043860\n",
            "epoch 132, train loss=0.299920, val loss=1.698006, train acc=0.684211, val acc=0.043860\n",
            "epoch 133, train loss=0.299257, val loss=1.693299, train acc=0.684211, val acc=0.043860\n",
            "epoch 134, train loss=0.298594, val loss=1.688724, train acc=0.684211, val acc=0.043860\n",
            "epoch 135, train loss=0.297934, val loss=1.684279, train acc=0.684211, val acc=0.043860\n",
            "epoch 136, train loss=0.297275, val loss=1.679955, train acc=0.684211, val acc=0.043860\n",
            "epoch 137, train loss=0.296618, val loss=1.675744, train acc=0.684211, val acc=0.043860\n",
            "epoch 138, train loss=0.295963, val loss=1.671607, train acc=0.684211, val acc=0.043860\n",
            "epoch 139, train loss=0.295312, val loss=1.667520, train acc=0.684211, val acc=0.043860\n",
            "epoch 140, train loss=0.294664, val loss=1.663469, train acc=0.684211, val acc=0.043860\n",
            "epoch 141, train loss=0.294018, val loss=1.659424, train acc=0.684211, val acc=0.043860\n",
            "epoch 142, train loss=0.293376, val loss=1.655375, train acc=0.684211, val acc=0.043860\n",
            "epoch 143, train loss=0.292737, val loss=1.651329, train acc=0.684211, val acc=0.043860\n",
            "epoch 144, train loss=0.292102, val loss=1.647260, train acc=0.684211, val acc=0.043860\n",
            "epoch 145, train loss=0.291471, val loss=1.643127, train acc=0.684211, val acc=0.043860\n",
            "epoch 146, train loss=0.290849, val loss=1.638900, train acc=0.684211, val acc=0.043860\n",
            "epoch 147, train loss=0.290229, val loss=1.634619, train acc=0.684211, val acc=0.043860\n",
            "epoch 148, train loss=0.289611, val loss=1.630285, train acc=0.684211, val acc=0.043860\n",
            "epoch 149, train loss=0.288997, val loss=1.625886, train acc=0.684211, val acc=0.043860\n",
            "epoch 150, train loss=0.288385, val loss=1.621474, train acc=0.684211, val acc=0.043860\n",
            "epoch 151, train loss=0.287774, val loss=1.617087, train acc=0.684211, val acc=0.043860\n",
            "epoch 152, train loss=0.287166, val loss=1.612751, train acc=0.684211, val acc=0.043860\n",
            "epoch 153, train loss=0.286559, val loss=1.608487, train acc=0.684211, val acc=0.043860\n",
            "epoch 154, train loss=0.285954, val loss=1.604296, train acc=0.684211, val acc=0.043860\n",
            "epoch 155, train loss=0.285351, val loss=1.600176, train acc=0.684211, val acc=0.043860\n",
            "epoch 156, train loss=0.284751, val loss=1.596122, train acc=0.684211, val acc=0.043860\n",
            "epoch 157, train loss=0.284152, val loss=1.592122, train acc=0.684211, val acc=0.043860\n",
            "epoch 158, train loss=0.283556, val loss=1.588166, train acc=0.684211, val acc=0.043860\n",
            "epoch 159, train loss=0.282961, val loss=1.584231, train acc=0.684211, val acc=0.043860\n",
            "epoch 160, train loss=0.282368, val loss=1.580314, train acc=0.684211, val acc=0.043860\n",
            "epoch 161, train loss=0.281778, val loss=1.576409, train acc=0.684211, val acc=0.043860\n",
            "epoch 162, train loss=0.281189, val loss=1.572516, train acc=0.684211, val acc=0.043860\n",
            "epoch 163, train loss=0.280602, val loss=1.568631, train acc=0.684211, val acc=0.043860\n",
            "epoch 164, train loss=0.280017, val loss=1.564753, train acc=0.684211, val acc=0.043860\n",
            "epoch 165, train loss=0.279433, val loss=1.560883, train acc=0.684211, val acc=0.043860\n",
            "epoch 166, train loss=0.278851, val loss=1.557023, train acc=0.684211, val acc=0.043860\n",
            "epoch 167, train loss=0.278271, val loss=1.553173, train acc=0.684211, val acc=0.043860\n",
            "epoch 168, train loss=0.277693, val loss=1.549336, train acc=0.684211, val acc=0.043860\n",
            "epoch 169, train loss=0.277116, val loss=1.545511, train acc=0.684211, val acc=0.043860\n",
            "epoch 170, train loss=0.276541, val loss=1.541702, train acc=0.684211, val acc=0.043860\n",
            "epoch 171, train loss=0.275967, val loss=1.537913, train acc=0.684211, val acc=0.043860\n",
            "epoch 172, train loss=0.275396, val loss=1.534143, train acc=0.684211, val acc=0.043860\n",
            "epoch 173, train loss=0.274826, val loss=1.530392, train acc=0.684211, val acc=0.043860\n",
            "epoch 174, train loss=0.274262, val loss=1.526595, train acc=0.684211, val acc=0.043860\n",
            "epoch 175, train loss=0.273700, val loss=1.522776, train acc=0.684211, val acc=0.043860\n",
            "epoch 176, train loss=0.273138, val loss=1.518958, train acc=0.684211, val acc=0.043860\n",
            "epoch 177, train loss=0.272576, val loss=1.515160, train acc=0.684211, val acc=0.043860\n",
            "epoch 178, train loss=0.272016, val loss=1.511396, train acc=0.684211, val acc=0.043860\n",
            "epoch 179, train loss=0.271457, val loss=1.507670, train acc=0.684211, val acc=0.043860\n",
            "epoch 180, train loss=0.270899, val loss=1.503989, train acc=0.684211, val acc=0.043860\n",
            "epoch 181, train loss=0.270343, val loss=1.500352, train acc=0.684211, val acc=0.043860\n",
            "epoch 182, train loss=0.269788, val loss=1.496763, train acc=0.684211, val acc=0.043860\n",
            "epoch 183, train loss=0.269236, val loss=1.493210, train acc=0.684211, val acc=0.043860\n",
            "epoch 184, train loss=0.268685, val loss=1.489696, train acc=0.684211, val acc=0.043860\n",
            "epoch 185, train loss=0.268135, val loss=1.486207, train acc=0.684211, val acc=0.043860\n",
            "epoch 186, train loss=0.267587, val loss=1.482732, train acc=0.684211, val acc=0.043860\n",
            "epoch 187, train loss=0.267041, val loss=1.479262, train acc=0.684211, val acc=0.043860\n",
            "epoch 188, train loss=0.266496, val loss=1.475792, train acc=0.684211, val acc=0.043860\n",
            "epoch 189, train loss=0.265949, val loss=1.472397, train acc=0.684211, val acc=0.043860\n",
            "epoch 190, train loss=0.265408, val loss=1.468986, train acc=0.684211, val acc=0.043860\n",
            "epoch 191, train loss=0.264869, val loss=1.465557, train acc=0.684211, val acc=0.043860\n",
            "epoch 192, train loss=0.264332, val loss=1.462114, train acc=0.684211, val acc=0.043860\n",
            "epoch 193, train loss=0.263791, val loss=1.458725, train acc=0.684211, val acc=0.043860\n",
            "epoch 194, train loss=0.263257, val loss=1.455304, train acc=0.684211, val acc=0.043860\n",
            "epoch 195, train loss=0.262724, val loss=1.451861, train acc=0.684211, val acc=0.043860\n",
            "epoch 196, train loss=0.262192, val loss=1.448405, train acc=0.684211, val acc=0.043860\n",
            "epoch 197, train loss=0.261661, val loss=1.444952, train acc=0.684211, val acc=0.043860\n",
            "epoch 198, train loss=0.261131, val loss=1.441517, train acc=0.684211, val acc=0.043860\n",
            "epoch 199, train loss=0.260600, val loss=1.438168, train acc=0.684211, val acc=0.043860\n",
            "epoch 200, train loss=0.260074, val loss=1.434816, train acc=0.684211, val acc=0.043860\n",
            "epoch 201, train loss=0.259550, val loss=1.431464, train acc=0.684211, val acc=0.043860\n",
            "epoch 202, train loss=0.259028, val loss=1.428115, train acc=0.684211, val acc=0.043860\n",
            "epoch 203, train loss=0.258507, val loss=1.424773, train acc=0.684211, val acc=0.043860\n",
            "epoch 204, train loss=0.257988, val loss=1.421446, train acc=0.684211, val acc=0.052632\n",
            "epoch 205, train loss=0.257467, val loss=1.418176, train acc=0.684211, val acc=0.052632\n",
            "epoch 206, train loss=0.256952, val loss=1.414839, train acc=0.684211, val acc=0.052632\n",
            "epoch 207, train loss=0.256439, val loss=1.411462, train acc=0.684211, val acc=0.052632\n",
            "epoch 208, train loss=0.255927, val loss=1.408069, train acc=0.684211, val acc=0.052632\n",
            "epoch 209, train loss=0.255417, val loss=1.404691, train acc=0.684211, val acc=0.052632\n",
            "epoch 210, train loss=0.254910, val loss=1.401343, train acc=0.684211, val acc=0.052632\n",
            "epoch 211, train loss=0.254400, val loss=1.398101, train acc=0.684211, val acc=0.052632\n",
            "epoch 212, train loss=0.253898, val loss=1.394876, train acc=0.684211, val acc=0.052632\n",
            "epoch 213, train loss=0.253397, val loss=1.391668, train acc=0.684211, val acc=0.052632\n",
            "epoch 214, train loss=0.252897, val loss=1.388464, train acc=0.694737, val acc=0.052632\n",
            "epoch 215, train loss=0.252399, val loss=1.385270, train acc=0.705263, val acc=0.052632\n",
            "epoch 216, train loss=0.251902, val loss=1.382092, train acc=0.705263, val acc=0.052632\n",
            "epoch 217, train loss=0.251408, val loss=1.378932, train acc=0.705263, val acc=0.052632\n",
            "epoch 218, train loss=0.250911, val loss=1.375874, train acc=0.705263, val acc=0.061404\n",
            "epoch 219, train loss=0.250420, val loss=1.372816, train acc=0.705263, val acc=0.061404\n",
            "epoch 220, train loss=0.249931, val loss=1.369756, train acc=0.705263, val acc=0.061404\n",
            "epoch 221, train loss=0.249443, val loss=1.366685, train acc=0.705263, val acc=0.070175\n",
            "epoch 222, train loss=0.248956, val loss=1.363608, train acc=0.705263, val acc=0.070175\n",
            "epoch 223, train loss=0.248471, val loss=1.360534, train acc=0.705263, val acc=0.070175\n",
            "epoch 224, train loss=0.247983, val loss=1.357550, train acc=0.705263, val acc=0.087719\n",
            "epoch 225, train loss=0.247502, val loss=1.354561, train acc=0.705263, val acc=0.087719\n",
            "epoch 226, train loss=0.247022, val loss=1.351556, train acc=0.705263, val acc=0.087719\n",
            "epoch 227, train loss=0.246542, val loss=1.348541, train acc=0.705263, val acc=0.087719\n",
            "epoch 228, train loss=0.246065, val loss=1.345539, train acc=0.705263, val acc=0.087719\n",
            "epoch 229, train loss=0.245588, val loss=1.342551, train acc=0.705263, val acc=0.087719\n",
            "epoch 230, train loss=0.245112, val loss=1.339575, train acc=0.705263, val acc=0.087719\n",
            "epoch 231, train loss=0.244636, val loss=1.336686, train acc=0.705263, val acc=0.087719\n",
            "epoch 232, train loss=0.244166, val loss=1.333788, train acc=0.715789, val acc=0.087719\n",
            "epoch 233, train loss=0.243697, val loss=1.330880, train acc=0.715789, val acc=0.105263\n",
            "epoch 234, train loss=0.243231, val loss=1.327978, train acc=0.715789, val acc=0.105263\n",
            "epoch 235, train loss=0.242765, val loss=1.325082, train acc=0.715789, val acc=0.114035\n",
            "epoch 236, train loss=0.242301, val loss=1.322182, train acc=0.715789, val acc=0.114035\n",
            "epoch 237, train loss=0.241838, val loss=1.319294, train acc=0.715789, val acc=0.114035\n",
            "epoch 238, train loss=0.241373, val loss=1.316491, train acc=0.715789, val acc=0.114035\n",
            "epoch 239, train loss=0.240916, val loss=1.313694, train acc=0.715789, val acc=0.114035\n",
            "epoch 240, train loss=0.240460, val loss=1.310896, train acc=0.715789, val acc=0.114035\n",
            "epoch 241, train loss=0.240006, val loss=1.308085, train acc=0.715789, val acc=0.122807\n",
            "epoch 242, train loss=0.239553, val loss=1.305268, train acc=0.715789, val acc=0.122807\n",
            "epoch 243, train loss=0.239101, val loss=1.302449, train acc=0.715789, val acc=0.131579\n",
            "epoch 244, train loss=0.238647, val loss=1.299710, train acc=0.715789, val acc=0.131579\n",
            "epoch 245, train loss=0.238199, val loss=1.296952, train acc=0.715789, val acc=0.131579\n",
            "epoch 246, train loss=0.237753, val loss=1.294193, train acc=0.715789, val acc=0.149123\n",
            "epoch 247, train loss=0.237307, val loss=1.291436, train acc=0.715789, val acc=0.149123\n",
            "epoch 248, train loss=0.236863, val loss=1.288675, train acc=0.715789, val acc=0.157895\n",
            "epoch 249, train loss=0.236419, val loss=1.285918, train acc=0.715789, val acc=0.166667\n",
            "epoch 250, train loss=0.235973, val loss=1.283244, train acc=0.715789, val acc=0.175439\n",
            "epoch 251, train loss=0.235533, val loss=1.280564, train acc=0.715789, val acc=0.175439\n",
            "epoch 252, train loss=0.235095, val loss=1.277871, train acc=0.715789, val acc=0.175439\n",
            "epoch 253, train loss=0.234657, val loss=1.275173, train acc=0.715789, val acc=0.175439\n",
            "epoch 254, train loss=0.234222, val loss=1.272488, train acc=0.715789, val acc=0.175439\n",
            "epoch 255, train loss=0.233786, val loss=1.269807, train acc=0.715789, val acc=0.175439\n",
            "epoch 256, train loss=0.233348, val loss=1.267221, train acc=0.715789, val acc=0.175439\n",
            "epoch 257, train loss=0.232916, val loss=1.264625, train acc=0.715789, val acc=0.175439\n",
            "epoch 258, train loss=0.232485, val loss=1.262028, train acc=0.715789, val acc=0.175439\n",
            "epoch 259, train loss=0.232057, val loss=1.259421, train acc=0.715789, val acc=0.184211\n",
            "epoch 260, train loss=0.231629, val loss=1.256821, train acc=0.715789, val acc=0.184211\n",
            "epoch 261, train loss=0.231203, val loss=1.254215, train acc=0.715789, val acc=0.184211\n",
            "epoch 262, train loss=0.230773, val loss=1.251681, train acc=0.715789, val acc=0.184211\n",
            "epoch 263, train loss=0.230351, val loss=1.249135, train acc=0.715789, val acc=0.184211\n",
            "epoch 264, train loss=0.229929, val loss=1.246574, train acc=0.715789, val acc=0.184211\n",
            "epoch 265, train loss=0.229508, val loss=1.244006, train acc=0.715789, val acc=0.184211\n",
            "epoch 266, train loss=0.229088, val loss=1.241427, train acc=0.715789, val acc=0.184211\n",
            "epoch 267, train loss=0.228669, val loss=1.238859, train acc=0.715789, val acc=0.184211\n",
            "epoch 268, train loss=0.228248, val loss=1.236382, train acc=0.715789, val acc=0.192982\n",
            "epoch 269, train loss=0.227833, val loss=1.233894, train acc=0.715789, val acc=0.192982\n",
            "epoch 270, train loss=0.227419, val loss=1.231397, train acc=0.715789, val acc=0.201754\n",
            "epoch 271, train loss=0.227006, val loss=1.228896, train acc=0.715789, val acc=0.201754\n",
            "epoch 272, train loss=0.226594, val loss=1.226396, train acc=0.715789, val acc=0.201754\n",
            "epoch 273, train loss=0.226179, val loss=1.223970, train acc=0.715789, val acc=0.201754\n",
            "epoch 274, train loss=0.225771, val loss=1.221531, train acc=0.715789, val acc=0.201754\n",
            "epoch 275, train loss=0.225364, val loss=1.219083, train acc=0.715789, val acc=0.201754\n",
            "epoch 276, train loss=0.224958, val loss=1.216631, train acc=0.715789, val acc=0.210526\n",
            "epoch 277, train loss=0.224553, val loss=1.214171, train acc=0.715789, val acc=0.228070\n",
            "epoch 278, train loss=0.224149, val loss=1.211722, train acc=0.715789, val acc=0.228070\n",
            "epoch 279, train loss=0.223742, val loss=1.209365, train acc=0.715789, val acc=0.228070\n",
            "epoch 280, train loss=0.223341, val loss=1.206994, train acc=0.715789, val acc=0.228070\n",
            "epoch 281, train loss=0.222941, val loss=1.204612, train acc=0.715789, val acc=0.228070\n",
            "epoch 282, train loss=0.222542, val loss=1.202222, train acc=0.726316, val acc=0.236842\n",
            "epoch 283, train loss=0.222144, val loss=1.199822, train acc=0.726316, val acc=0.245614\n",
            "epoch 284, train loss=0.221743, val loss=1.197506, train acc=0.726316, val acc=0.245614\n",
            "epoch 285, train loss=0.221349, val loss=1.195171, train acc=0.726316, val acc=0.245614\n",
            "epoch 286, train loss=0.220956, val loss=1.192820, train acc=0.726316, val acc=0.254386\n",
            "epoch 287, train loss=0.220563, val loss=1.190452, train acc=0.726316, val acc=0.254386\n",
            "epoch 288, train loss=0.220172, val loss=1.188087, train acc=0.726316, val acc=0.254386\n",
            "epoch 289, train loss=0.219781, val loss=1.185733, train acc=0.726316, val acc=0.263158\n",
            "epoch 290, train loss=0.219388, val loss=1.183471, train acc=0.726316, val acc=0.271930\n",
            "epoch 291, train loss=0.219001, val loss=1.181194, train acc=0.726316, val acc=0.271930\n",
            "epoch 292, train loss=0.218615, val loss=1.178895, train acc=0.726316, val acc=0.271930\n",
            "epoch 293, train loss=0.218230, val loss=1.176593, train acc=0.726316, val acc=0.280702\n",
            "epoch 294, train loss=0.217846, val loss=1.174295, train acc=0.726316, val acc=0.280702\n",
            "epoch 295, train loss=0.217459, val loss=1.172080, train acc=0.726316, val acc=0.289474\n",
            "epoch 296, train loss=0.217078, val loss=1.169838, train acc=0.736842, val acc=0.289474\n",
            "epoch 297, train loss=0.216699, val loss=1.167585, train acc=0.736842, val acc=0.289474\n",
            "epoch 298, train loss=0.216320, val loss=1.165329, train acc=0.736842, val acc=0.289474\n",
            "epoch 299, train loss=0.215942, val loss=1.163064, train acc=0.736842, val acc=0.289474\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuClass": "premium",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}