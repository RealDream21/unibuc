{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37e04400-b895-4194-9619-b054a3dc5fa3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Fabi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk #biblioteca pentru cuvinte\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "english_words = stopwords.words('english')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d00a50a-6b60-4b95-a04a-389040869b3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"enronSpamSubset.csv\")\n",
    "data = data.fillna(\"empty\")\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.2)\n",
    "train_data = train_data.fillna(\"empty\")\n",
    "test_data = test_data.fillna(\"empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e26695d-a302-4d01-a6d7-f4abcd6410b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Subject', ':', 'are', 'you', 'listed', 'in', 'major', 'search', 'engines', '?', 'submitting', 'your', 'website', 'in', 'search', 'engines', 'may', 'increase', 'your', 'online', 'sales', 'dramatically', '.', 'lf', 'you', 'invested', 'time', 'and', 'money', 'into', 'your', 'website', ',', 'you', 'simply', 'must', 'submit', 'your', 'website', 'oniine', 'otherwise', 'it', 'wili', 'be', 'invisible', 'virtuaiiy', ',', 'which', 'means', 'efforts', 'spent', 'in', 'vain', '.', 'lf', 'you', 'want', 'peopie', 'to', 'know', 'about', 'your', 'website', 'and', 'boost', 'your', 'revenues', ',', 'the', 'only', 'way', 'to', 'do', 'that', 'is', 'to', 'make', 'your', 'site', 'visible', 'in', 'places', 'where', 'people', 'search', 'for', 'information', ',', 'i', '.', 'e', '.', 'submit', 'your', 'website', 'in', 'multipie', 'search', 'engines', '.', 'submit', 'your', 'website', 'online', 'and', 'watch', 'visitors', 'stream', 'to', 'your', 'e', '-', 'business', '.', 'best', 'reqards', ',', 'brittnyatkins', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'not', 'interested', '.', '.', '.', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_']\n",
      "_______\n",
      "['subject', ':', 'are', 'you', 'listed', 'in', 'major', 'search', 'engines', '?', 'submitting', 'your', 'website', 'in', 'search', 'engines', 'may', 'increase', 'your', 'online', 'sales', 'dramatically', '.', 'lf', 'you', 'invested', 'time', 'and', 'money', 'into', 'your', 'website', ',', 'you', 'simply', 'must', 'submit', 'your', 'website', 'oniine', 'otherwise', 'it', 'wili', 'be', 'invisible', 'virtuaiiy', ',', 'which', 'means', 'efforts', 'spent', 'in', 'vain', '.', 'lf', 'you', 'want', 'peopie', 'to', 'know', 'about', 'your', 'website', 'and', 'boost', 'your', 'revenues', ',', 'the', 'only', 'way', 'to', 'do', 'that', 'is', 'to', 'make', 'your', 'site', 'visible', 'in', 'places', 'where', 'people', 'search', 'for', 'information', ',', 'i', '.', 'e', '.', 'submit', 'your', 'website', 'in', 'multipie', 'search', 'engines', '.', 'submit', 'your', 'website', 'online', 'and', 'watch', 'visitors', 'stream', 'to', 'your', 'e', '-', 'business', '.', 'best', 'reqards', ',', 'brittnyatkins', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'not', 'interested', '.', '.', '.', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_']\n",
      "_______\n",
      "['subject', ':', 'are', 'you', 'listed', 'in', 'major', 'search', 'engines', '?', 'submitting', 'your', 'website', 'in', 'search', 'engines', 'may', 'increase', 'your', 'online', 'sales', 'dramatically', '.', 'lf', 'you', 'invested', 'time', 'and', 'money', 'into', 'your', 'website', ',', 'you', 'simply', 'must', 'submit', 'your', 'website', 'oniine', 'otherwise', 'it', 'wili', 'be', 'invisible', 'virtuaiiy', ',', 'which', 'means', 'efforts', 'spent', 'in', 'vain', '.', 'lf', 'you', 'want', 'peopie', 'to', 'know', 'about', 'your', 'website', 'and', 'boost', 'your', 'revenues', ',', 'the', 'only', 'way', 'to', 'do', 'that', 'is', 'to', 'make', 'your', 'site', 'visible', 'in', 'places', 'where', 'people', 'search', 'for', 'information', ',', 'i', '.', 'e', '.', 'submit', 'your', 'website', 'in', 'multipie', 'search', 'engines', '.', 'submit', 'your', 'website', 'online', 'and', 'watch', 'visitors', 'stream', 'to', 'your', 'e', '-', 'business', '.', 'best', 'reqards', ',', 'brittnyatkins', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'not', 'interested', '.', '.', '.', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_']\n",
      "_______\n",
      "['subject', 'listed', 'major', 'search', 'engines', 'submitting', 'website', 'search', 'engines', 'may', 'increase', 'online', 'sales', 'dramatically', 'lf', 'invested', 'time', 'money', 'website', 'simply', 'must', 'submit', 'website', 'oniine', 'otherwise', 'wili', 'invisible', 'virtuaiiy', 'means', 'efforts', 'spent', 'vain', 'lf', 'want', 'peopie', 'know', 'website', 'boost', 'revenues', 'way', 'make', 'site', 'visible', 'places', 'people', 'search', 'information', 'e', 'submit', 'website', 'multipie', 'search', 'engines', 'submit', 'website', 'online', 'watch', 'visitors', 'stream', 'e', 'business', 'best', 'reqards', 'brittnyatkins', 'interested']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "cuvinte_curate = wordpunct_tokenize(data.iloc[1]['Body'])\n",
    "print(cuvinte_curate)\n",
    "cuvinte_curate = [cuv.lower() for cuv in cuvinte_curate]\n",
    "print(\"_______\")\n",
    "print(cuvinte_curate)\n",
    "print(\"_______\")\n",
    "cuvinte_curate = ['[num]' if cuv.isdigit() else cuv for cuv in cuvinte_curate]\n",
    "print(cuvinte_curate)\n",
    "\n",
    "\n",
    "def curatare_text(text_body):\n",
    "    # am putea sa le punem in functii separate\n",
    "    cuvinte_curate = wordpunct_tokenize(data.iloc[1]['Body'])\n",
    "    cuvinte_curate = [cuv.lower() for cuv in cuvinte_curate]\n",
    "    cuvinte_curate = ['[num]' if cuv.isdigit() else cuv for cuv in cuvinte_curate]\n",
    "    cuvinte_curate = [cuv for cuv in cuvinte_curate if cuv not in english_words]\n",
    "    cuvinte_curate = [cuv for cuv in cuvinte_curate if cuv not in string.punctuation]\n",
    "    return cuvinte_curate\n",
    "\n",
    "\n",
    "print(\"_______\")\n",
    "print(curatare_text(data.iloc[1]['Body']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8e8d1d7b-de7a-44f2-a093-dfa915fb239b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "class CLASIFICATORUL():\n",
    "    def __init__(self, train_data):\n",
    "        self.data = train_data\n",
    "        self.label_map = {\n",
    "            0: \"NON-SPAM\",\n",
    "            1: \"SPAM\",\n",
    "        }\n",
    "        self.apriori_probs = {\n",
    "            1: 0.05,\n",
    "            0: 0.95,\n",
    "        }\n",
    "        self.spam_words = []\n",
    "        self.ham_words = []\n",
    "        \n",
    "        self.prob_maps = [\n",
    "            {} for i in range(len(self.label_map))\n",
    "        ]\n",
    "    \n",
    "    def clean_text(self, body):\n",
    "        cuvinte_curate = wordpunct_tokenize(body)\n",
    "        cuvinte_curate = [cuv.lower() for cuv in cuvinte_curate]\n",
    "        cuvinte_curate = ['[num]' if cuv.isdigit() else cuv for cuv in cuvinte_curate]\n",
    "        cuvinte_curate = [cuv for cuv in cuvinte_curate if cuv not in english_words]\n",
    "        cuvinte_curate = [cuv for cuv in cuvinte_curate if cuv not in string.punctuation]\n",
    "        return cuvinte_curate\n",
    "        \n",
    "    \n",
    "    \n",
    "    def _prepare_data(self):\n",
    "        self.spam_words = []\n",
    "        self.ham_words = []\n",
    "        print(\"PREPARING DATA....\")\n",
    "        for key, row in tqdm(self.data.iterrows()):\n",
    "            body = row['Body']\n",
    "            label = row['Label']\n",
    "            \n",
    "            words = self.clean_text(body)\n",
    "            if label == 1:\n",
    "                self.spam_words += words\n",
    "            else:\n",
    "                self.ham_words += words\n",
    "        \n",
    "        print(\"DONE..\")\n",
    "        \n",
    "    def build_prob_map(self, word_list, label):\n",
    "        \n",
    "        self.prob_maps[label] = {}\n",
    "        \n",
    "        c = Counter(word_list)\n",
    "        total_words_no = sum(c.values())\n",
    "        \n",
    "        for w, w_count in c.items():\n",
    "            self.prob_maps[label][w] = (w_count+1)/(total_words_no + 2)\n",
    "            \n",
    "    def compute_log_prob(self, words, label):\n",
    "        \n",
    "        log_prob = 0\n",
    "        unknown_words = 0\n",
    "        \n",
    "        for w in words:\n",
    "            if w in self.prob_maps[label]:\n",
    "                log_prob += np.log(self.prob_maps[label][w])\n",
    "            else:\n",
    "                log_prob += np.log(1e-7)\n",
    "                unknown_words += 1\n",
    "                \n",
    "        log_prob += np.log(self.apriori_probs[label])\n",
    "\n",
    "        return log_prob, unknown_words\n",
    "            \n",
    "            \n",
    "            \n",
    "    def fit(self):\n",
    "        self.build_prob_map(self.ham_words, 0)\n",
    "        self.build_prob_map(self.spam_words, 1)\n",
    "    \n",
    "    def inference(self, text):\n",
    "        words = self.clean_text(text)\n",
    "        unknown = []\n",
    "        result = []\n",
    "        for i in [0,1]:\n",
    "            log_prob, unknown_words = self.compute_log_prob(words, i)\n",
    "            result.append(log_prob)\n",
    "            unknown.append(unknown_words)\n",
    "            \n",
    "        return result, unknown\n",
    "    \n",
    "    def predict(self, text):\n",
    "        logs, unknown = self.inference(text)\n",
    "        return np.argmax(logs), unknown\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "61299bf1-10c1-4402-a619-74fd24acb3c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPARING DATA....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8000it [00:20, 392.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE..\n"
     ]
    }
   ],
   "source": [
    "spam_classifier = CLASIFICATORUL(train_data)\n",
    "spam_classifier._prepare_data()\n",
    "spam_classifier.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e75b483e-5f39-493c-9a0b-b71a86bf0901",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, [4, 3])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_classifier.predict(\"I am a nigger prince. Please giff credit card so I can send you much money!! Please send details. Offer!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "21268576-2d97-40da-912d-2f8d44c35c0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, testing_data):\n",
    "    \n",
    "    from sklearn.metrics import  classification_report\n",
    "    \n",
    "    good = 0\n",
    "    labels = []\n",
    "    predictions = []\n",
    "    unknowns = []\n",
    "    for key, row in tqdm(testing_data.iterrows()):\n",
    "        body = row['Body']\n",
    "        label = row['Label']\n",
    "        labels.append(label)\n",
    "        \n",
    "        prediction, unknown = spam_classifier.predict(body)\n",
    "        predictions.append(prediction)\n",
    "        unknowns.append(unknown)\n",
    "    \n",
    "    print(classification_report(labels, predictions))\n",
    "    \n",
    "    return unknowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8a59c772-39e3-4c4c-8892-49ce152ff859",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:07, 252.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98       988\n",
      "           1       0.99      0.96      0.98      1012\n",
      "\n",
      "    accuracy                           0.98      2000\n",
      "   macro avg       0.98      0.98      0.98      2000\n",
      "weighted avg       0.98      0.98      0.98      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "unknowns = evaluate_model(spam_classifier, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90408a0c-bac2-44de-b4d6-9253bb2f3d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
