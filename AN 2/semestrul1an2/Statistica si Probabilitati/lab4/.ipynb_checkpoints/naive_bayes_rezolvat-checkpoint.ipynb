{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afcb0184-8e2d-4938-89a1-9ccb2543d500",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ctgio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'DATA HERE: https://www.kaggle.com/nitishabharathi/email-spam-dataset/code'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "english_words = stopwords.words('english')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\"DATA HERE: https://www.kaggle.com/nitishabharathi/email-spam-dataset/code\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da34ab17-3199-4077-9c23-1854cb27a3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# english_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46de1036-6d3e-4b76-81f6-e6b2ef523221",
   "metadata": {},
   "source": [
    "## Citim datele si ne uitam prin ele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7bbd367-ff1e-423b-bbd3-4fb276c640b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"enronSpamSubset.csv\")\n",
    "data = data.fillna(\"empty\")\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.2)\n",
    "train_data = train_data.fillna(\"empty\")\n",
    "test_data = test_data.fillna(\"empty\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4394db70-6f03-4b35-9290-69052caef39e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Body</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>16111</td>\n",
       "      <td>16111</td>\n",
       "      <td>Subject: i didn ' t waitt any longer\\n hello ,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5306</th>\n",
       "      <td>22755</td>\n",
       "      <td>22755</td>\n",
       "      <td>Subject: re : swerzbin has signed ! ! !\\n don ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3271</th>\n",
       "      <td>9765</td>\n",
       "      <td>9765</td>\n",
       "      <td>Subject: visual identity and logo now\\n workin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7192</th>\n",
       "      <td>26908</td>\n",
       "      <td>26908</td>\n",
       "      <td>Subject: revised spreadsheet\\n new additions .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1736</th>\n",
       "      <td>15914</td>\n",
       "      <td>15914</td>\n",
       "      <td>Subject: ge of the criminal goods and services...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6232</th>\n",
       "      <td>31299</td>\n",
       "      <td>31299</td>\n",
       "      <td>Subject: california update 3 / 15 / 01\\n execu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8368</th>\n",
       "      <td>19436</td>\n",
       "      <td>19436</td>\n",
       "      <td>Subject: moneycentral : 6 routes to retire ric...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5884</th>\n",
       "      <td>20864</td>\n",
       "      <td>20864</td>\n",
       "      <td>Subject: trip plans\\n hi cindy and sally ,\\n b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>12774</td>\n",
       "      <td>12774</td>\n",
       "      <td>Subject: real - time online trade !\\n hello an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>7641</td>\n",
       "      <td>7641</td>\n",
       "      <td>Subject: spend your money where it counts .\\n ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  Unnamed: 0.1  \\\n",
       "1997       16111         16111   \n",
       "5306       22755         22755   \n",
       "3271        9765          9765   \n",
       "7192       26908         26908   \n",
       "1736       15914         15914   \n",
       "...          ...           ...   \n",
       "6232       31299         31299   \n",
       "8368       19436         19436   \n",
       "5884       20864         20864   \n",
       "2504       12774         12774   \n",
       "2695        7641          7641   \n",
       "\n",
       "                                                   Body  Label  \n",
       "1997  Subject: i didn ' t waitt any longer\\n hello ,...      1  \n",
       "5306  Subject: re : swerzbin has signed ! ! !\\n don ...      0  \n",
       "3271  Subject: visual identity and logo now\\n workin...      1  \n",
       "7192     Subject: revised spreadsheet\\n new additions .      0  \n",
       "1736  Subject: ge of the criminal goods and services...      1  \n",
       "...                                                 ...    ...  \n",
       "6232  Subject: california update 3 / 15 / 01\\n execu...      0  \n",
       "8368  Subject: moneycentral : 6 routes to retire ric...      0  \n",
       "5884  Subject: trip plans\\n hi cindy and sally ,\\n b...      0  \n",
       "2504  Subject: real - time online trade !\\n hello an...      1  \n",
       "2695  Subject: spend your money where it counts .\\n ...      1  \n",
       "\n",
       "[8000 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data.iloc[0]['Body']\n",
    "# data.isnull().values.any()\n",
    "# data = data.fillna(\"empty\")\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787df08b-f39d-4e0b-ac1d-c6367bbc0aec",
   "metadata": {},
   "source": [
    "## Pipeline de preprocesare (curatam textul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e56194a7-8e53-44d3-9076-5e9fda7bee7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Subject', ':', 'are', 'you', 'listed', 'in', 'major', 'search', 'engines', '?', 'submitting', 'your', 'website', 'in', 'search', 'engines', 'may', 'increase', 'your', 'online', 'sales', 'dramatically', '.', 'lf', 'you', 'invested', 'time', 'and', 'money', 'into', 'your', 'website', ',', 'you', 'simply', 'must', 'submit', 'your', 'website', 'oniine', 'otherwise', 'it', 'wili', 'be', 'invisible', 'virtuaiiy', ',', 'which', 'means', 'efforts', 'spent', 'in', 'vain', '.', 'lf', 'you', 'want', 'peopie', 'to', 'know', 'about', 'your', 'website', 'and', 'boost', 'your', 'revenues', ',', 'the', 'only', 'way', 'to', 'do', 'that', 'is', 'to', 'make', 'your', 'site', 'visible', 'in', 'places', 'where', 'people', 'search', 'for', 'information', ',', 'i', '.', 'e', '.', 'submit', 'your', 'website', 'in', 'multipie', 'search', 'engines', '.', 'submit', 'your', 'website', 'online', 'and', 'watch', 'visitors', 'stream', 'to', 'your', 'e', '-', 'business', '.', 'best', 'reqards', ',', 'brittnyatkins', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'not', 'interested', '.', '.', '.', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_']\n",
      "_______\n",
      "['subject', ':', 'are', 'you', 'listed', 'in', 'major', 'search', 'engines', '?', 'submitting', 'your', 'website', 'in', 'search', 'engines', 'may', 'increase', 'your', 'online', 'sales', 'dramatically', '.', 'lf', 'you', 'invested', 'time', 'and', 'money', 'into', 'your', 'website', ',', 'you', 'simply', 'must', 'submit', 'your', 'website', 'oniine', 'otherwise', 'it', 'wili', 'be', 'invisible', 'virtuaiiy', ',', 'which', 'means', 'efforts', 'spent', 'in', 'vain', '.', 'lf', 'you', 'want', 'peopie', 'to', 'know', 'about', 'your', 'website', 'and', 'boost', 'your', 'revenues', ',', 'the', 'only', 'way', 'to', 'do', 'that', 'is', 'to', 'make', 'your', 'site', 'visible', 'in', 'places', 'where', 'people', 'search', 'for', 'information', ',', 'i', '.', 'e', '.', 'submit', 'your', 'website', 'in', 'multipie', 'search', 'engines', '.', 'submit', 'your', 'website', 'online', 'and', 'watch', 'visitors', 'stream', 'to', 'your', 'e', '-', 'business', '.', 'best', 'reqards', ',', 'brittnyatkins', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'not', 'interested', '.', '.', '.', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_']\n",
      "_______\n",
      "['subject', ':', 'are', 'you', 'listed', 'in', 'major', 'search', 'engines', '?', 'submitting', 'your', 'website', 'in', 'search', 'engines', 'may', 'increase', 'your', 'online', 'sales', 'dramatically', '.', 'lf', 'you', 'invested', 'time', 'and', 'money', 'into', 'your', 'website', ',', 'you', 'simply', 'must', 'submit', 'your', 'website', 'oniine', 'otherwise', 'it', 'wili', 'be', 'invisible', 'virtuaiiy', ',', 'which', 'means', 'efforts', 'spent', 'in', 'vain', '.', 'lf', 'you', 'want', 'peopie', 'to', 'know', 'about', 'your', 'website', 'and', 'boost', 'your', 'revenues', ',', 'the', 'only', 'way', 'to', 'do', 'that', 'is', 'to', 'make', 'your', 'site', 'visible', 'in', 'places', 'where', 'people', 'search', 'for', 'information', ',', 'i', '.', 'e', '.', 'submit', 'your', 'website', 'in', 'multipie', 'search', 'engines', '.', 'submit', 'your', 'website', 'online', 'and', 'watch', 'visitors', 'stream', 'to', 'your', 'e', '-', 'business', '.', 'best', 'reqards', ',', 'brittnyatkins', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'not', 'interested', '.', '.', '.', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_']\n",
      "_______\n",
      "['subject', ':', 'are', 'you', 'listed', 'in', 'major', 'search', 'engines', '?', 'submitting', 'your', 'website', 'in', 'search', 'engines', 'may', 'increase', 'your', 'online', 'sales', 'dramatically', '.', 'lf', 'you', 'invested', 'time', 'and', 'money', 'into', 'your', 'website', ',', 'you', 'simply', 'must', 'submit', 'your', 'website', 'oniine', 'otherwise', 'it', 'wili', 'be', 'invisible', 'virtuaiiy', ',', 'which', 'means', 'efforts', 'spent', 'in', 'vain', '.', 'lf', 'you', 'want', 'peopie', 'to', 'know', 'about', 'your', 'website', 'and', 'boost', 'your', 'revenues', ',', 'the', 'only', 'way', 'to', 'do', 'that', 'is', 'to', 'make', 'your', 'site', 'visible', 'in', 'places', 'where', 'people', 'search', 'for', 'information', ',', 'i', '.', 'e', '.', 'submit', 'your', 'website', 'in', 'multipie', 'search', 'engines', '.', 'submit', 'your', 'website', 'online', 'and', 'watch', 'visitors', 'stream', 'to', 'your', 'e', '-', 'business', '.', 'best', 'reqards', ',', 'brittnyatkins', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'not', 'interested', '.', '.', '.', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "cuvinte_curate = wordpunct_tokenize(data.iloc[1]['Body'])\n",
    "print(cuvinte_curate)\n",
    "cuvinte_curate = [cuv.lower() for cuv in cuvinte_curate]\n",
    "print(\"_______\")\n",
    "print(cuvinte_curate)\n",
    "print(\"_______\")\n",
    "cuvinte_curate = ['[num]' if cuv.isdigit() else cuv for cuv in cuvinte_curate]\n",
    "print(cuvinte_curate)\n",
    "\n",
    "\n",
    "def curatare_text(text_body):\n",
    "    # am putea sa le punem in functii separate\n",
    "    cuvinte_curate = wordpunct_tokenize(data.iloc[1]['Body'])\n",
    "    cuvinte_curate = [cuv.lower() for cuv in cuvinte_curate]\n",
    "    cuvinte_curate = ['[num]' if cuv.isdigit() else cuv for cuv in cuvinte_curate]\n",
    "    \n",
    "    return cuvinte_curate\n",
    "\n",
    "\n",
    "print(\"_______\")\n",
    "print(curatare_text(data.iloc[1]['Body']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fb5581-a673-4161-b36c-e2387c2f59a4",
   "metadata": {},
   "source": [
    "### Clasificatorul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91523fe1-2bcc-4d23-a2c9-b69e8467ca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "class SpamBayesianClassifier():\n",
    "    \n",
    "    def __init__(self, data, labels_no):\n",
    "        \n",
    "        self.data = data\n",
    "        self.labels_no = labels_no\n",
    "        self.spam_words = []\n",
    "        self.ham_words = []\n",
    "        self.prob_maps = [{} for _ in range(labels_no)]\n",
    "        self.prior = [1/labels_no for _ in range(labels_no)]\n",
    "    \n",
    "    def _prepare_data(self):\n",
    "        self.spam_words = []\n",
    "        self.ham_words = []\n",
    "        print(\"PREPARING DATA....\")\n",
    "        for key, row in tqdm(self.data.iterrows()):\n",
    "            body = row['Body']\n",
    "            label = row['Label']\n",
    "            \n",
    "            words = self.clean_text(body)\n",
    "            if label == 1:\n",
    "                self.spam_words += words\n",
    "            else:\n",
    "                self.ham_words += words\n",
    "        \n",
    "        print(\"DONE..\")\n",
    "        \n",
    "    def _tokenize_text(self, text_body):\n",
    "        from nltk.tokenize import wordpunct_tokenize\n",
    "        \n",
    "        # print(text_body)\n",
    "        return wordpunct_tokenize(text_body)\n",
    "    \n",
    "    def _lowercase(self, words):\n",
    "        \n",
    "        return [w.lower() for w in words]\n",
    "    \n",
    "    def _replace_nums(self, words):\n",
    "        \n",
    "        return ['[num]' if w.isdigit() else w for w in words]\n",
    "    \n",
    "    ### NEW STUFF\n",
    "    def _remove_punctuation(self, words):\n",
    "        \"\"\"\n",
    "        Aceasta functie scoate semnele de punctuatie din lista de cuvinte. Pastrez doar\n",
    "        cuvintele care nu sunt in lista de semne de punctuatie. (',./.,?[]..etc')\n",
    "        \"\"\"\n",
    "        return [w for w in words if w not in string.punctuation]\n",
    "    \n",
    "    \n",
    "    def _remove_stop_words(self, words):\n",
    "        \"\"\"\n",
    "        Scoatem din lista de cuvinte, cuvintele care nu au insemnatate. De exemplu,\n",
    "        cuvinte de legatura, preprozitii, etc. In pachetul nltk, acestea se numesc 'stopwords'\n",
    "        \"\"\"\n",
    "        \n",
    "        return [w for w in words if w not in english_words]\n",
    "    ### NEW STUFF\n",
    "    \n",
    "    def clean_text(self, text_body):\n",
    "        \n",
    "        raw_words = self._tokenize_text(text_body)\n",
    "        lowercase_words = self._lowercase(raw_words)\n",
    "        no_punctuation = self._remove_punctuation(lowercase_words)\n",
    "        no_stop_words = self._remove_stop_words(no_punctuation)\n",
    "        \n",
    "        return self._replace_nums(no_stop_words)\n",
    "    \n",
    "    def print_data_stats(self):\n",
    "        \n",
    "        no_ham_words = len(self.ham_words)\n",
    "        no_spam_words = len(self.spam_words)\n",
    "        total = no_ham_words + no_spam_words\n",
    "        \n",
    "        print(\"Ham words: \", no_ham_words/total)\n",
    "        print(\"Spam words: \", no_spam_words/total)\n",
    "        \n",
    "        return no_ham_words, no_spam_words, total\n",
    "    \n",
    "    # def unk_words(self, word_list):\n",
    "        # c = Counter(word_list)\n",
    "        # new_word_list = [w if c[w] >= 15 else '[unk]' for w in word_list]\n",
    "        \n",
    "        # return new_word_list\n",
    "    \n",
    "    def build_prob_map(self, word_list, label):\n",
    "        \n",
    "        self.prob_maps[label] = {}\n",
    "        \n",
    "        c = Counter(word_list)\n",
    "        total_words_no = sum(c.values())\n",
    "        \n",
    "        for w, w_count in c.items():\n",
    "            self.prob_maps[label][w] = (w_count+1)/(total_words_no + 2)\n",
    "\n",
    "    def fit(self):\n",
    "        \n",
    "        self.build_prob_map(self.ham_words, 0)\n",
    "        self.build_prob_map(self.spam_words, 1)\n",
    "    \n",
    "    def compute_log_prob(self, words, label):\n",
    "        \n",
    "        log_prob = 0\n",
    "        unknown_words = 0\n",
    "        \n",
    "        for w in words:\n",
    "            if w in self.prob_maps[label]:\n",
    "                log_prob += np.log(self.prob_maps[label][w])\n",
    "            else:\n",
    "                log_prob += np.log(1e-7)\n",
    "                unknown_words += 1\n",
    "                \n",
    "        log_prob += np.log(self.prior[label])\n",
    "        \n",
    "        return log_prob, unknown_words\n",
    "        \n",
    "    def inference(self, text):\n",
    "        words = self.clean_text(text)\n",
    "        unknown = []\n",
    "        result = []\n",
    "        for i in range(self.labels_no):\n",
    "            log_prob, unknown_words = self.compute_log_prob(words, i)\n",
    "            result.append(log_prob)\n",
    "            unknown.append(unknown_words)\n",
    "            \n",
    "        return result, unknown\n",
    "    \n",
    "    def predict(self, text):\n",
    "        \n",
    "        logs, unknown = self.inference(text)\n",
    "        \n",
    "        return np.argmax(logs), unknown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dabe680e-6cf5-4db3-9c55-4c7035eb2751",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [00:00, 1649.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPARING DATA....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8000it [00:05, 1510.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE..\n",
      "Ham words:  0.5682164810993587\n",
      "Spam words:  0.4317835189006413\n"
     ]
    }
   ],
   "source": [
    "spam_classifier = SpamBayesianClassifier(train_data, 2)\n",
    "spam_classifier._prepare_data()\n",
    "spam_classifier.print_data_stats()\n",
    "spam_classifier.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2a15d9f-7846-4136-a6c3-54d8fd5408f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, [3, 3])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_classifier.predict(\"I am a nigger prince. Please giff credit card so I can send you much money!! Please send details. Offer!\")\n",
    "# spam_classifier.prob_maps[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88efe0c-4350-4c65-8534-2ba786a715ba",
   "metadata": {},
   "source": [
    "### Random Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "896ed050-332b-4d04-a8d9-3ce9cffecaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spam_classifier.prob_maps[0]['[unk]']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10345c34-6c38-405e-ba02-6f1dd4a3b4a6",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3dee14b1-5a65-48ed-ae58-0ba30b317b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, testing_data):\n",
    "    \n",
    "    from sklearn.metrics import  classification_report\n",
    "    \n",
    "    good = 0\n",
    "    labels = []\n",
    "    predictions = []\n",
    "    unknowns = []\n",
    "    for key, row in tqdm(testing_data.iterrows()):\n",
    "        body = row['Body']\n",
    "        label = row['Label']\n",
    "        labels.append(label)\n",
    "        \n",
    "        prediction, unknown = spam_classifier.predict(body)\n",
    "        predictions.append(prediction)\n",
    "        unknowns.append(unknown)\n",
    "    \n",
    "    print(classification_report(labels, predictions))\n",
    "    \n",
    "    return unknowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fc1851c-5cb4-45b8-947a-18e484661896",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:02, 919.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      1027\n",
      "           1       0.98      0.97      0.98       973\n",
      "\n",
      "    accuracy                           0.98      2000\n",
      "   macro avg       0.98      0.98      0.98      2000\n",
      "weighted avg       0.98      0.98      0.98      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "unknowns = evaluate_model(spam_classifier, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc4e5f08-cb1a-4dda-90db-e86a0fee189e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-976582abb41e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munknowns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munknowns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.hist(np.array(unknowns)[:,0], bins=250)\n",
    "plt.show()\n",
    "plt.hist(np.array(unknowns)[:,1], bins=250)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
