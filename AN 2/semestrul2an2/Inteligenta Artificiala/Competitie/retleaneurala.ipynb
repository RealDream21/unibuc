{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7065075d-a692-4b81-9655-116122b24a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41da94f7-016a-46c4-b664-589f23b96ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv(\"train.csv\")\n",
    "test_labels = pd.read_csv(\"test.csv\")\n",
    "validation_labels = pd.read_csv(\"validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23098a51-ab66-427a-b0b1-0d57b8743104",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, labels, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(labels)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    @staticmethod\n",
    "    def read_image(img_path):\n",
    "        imagine = np.array(Image.open(img_path))\n",
    "        imagine = np.ravel(imagine.copy())\n",
    "        if(imagine.shape == (6400, )):\n",
    "            imagine = np.concatenate((imagine, imagine, imagine))\n",
    "        imagine = np.reshape(imagine, (80, 80, 3))\n",
    "        return np.reshape(imagine, (80, 80, 3))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0]) + \".png\"\n",
    "        #print(img_path)\n",
    "        image = self.read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c23fdf64-f488-4bcb-9703-beec3d4a830c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CustomImageDataset(\"train.csv\", \"train\", transform = ToTensor())\n",
    "test_data = CustomImageDataset(\"test.csv\", \"test\", transform = ToTensor())\n",
    "validation_data = CustomImageDataset(\"validation.csv\", \"validation\", transform = ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0ef1f01-8669-438c-90b6-cae8430b2b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReteaNeuronala(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.first_layer = nn.Linear(80 * 80 * 3, 512)\n",
    "        self.second_layer = nn.Linear(512, 512)\n",
    "        self.output_layer = nn.Linear(512, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.first_layer(x))\n",
    "        x = F.relu(self.second_layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3f3fba4-1b94-44a4-ab0f-8c5f8fb8a53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size = 64)\n",
    "test_loader = DataLoader(test_data, batch_size = 64)\n",
    "validation_loader = DataLoader(validation_data, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a228eaa6-a69e-4f05-bb1c-38ba3183562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creem retea si definim algoritmul de optimizare\n",
    "model = ReteaNeuronala()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9f0ace3-54b0-4f08-bae1-cf04d84c2d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "=== Epoch1===\n",
      "Batch index0, loss:0.977840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 100\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "model = model.to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "model.train(True)\n",
    "for i in range(NUM_EPOCHS):\n",
    "    print(f\"=== Epoch{i + 1}===\")\n",
    "    for batch, (image_batch, labels_batch) in enumerate(train_loader):\n",
    "        image_batch = image_batch.to(device)\n",
    "        labels_batch = labels_batch.to(device)\n",
    "        #print(image_batch.shape)\n",
    "        #print(labels_batch.shape)\n",
    "\n",
    "        pred = model(image_batch)\n",
    "        #print(pred.shape)\n",
    "        loss = loss_function(pred, labels_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch%100 == 0:\n",
    "            loss = loss.item()\n",
    "            print(f\"Batch index{batch}, loss:{loss:>7f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9cb5d8d9-0748-4e56-823b-1dcdab7ba266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 39.5%, Loss: 0.018307 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "test_loss = 0\n",
    "size = len(validation_loader.dataset)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for image_batch, labels_batch in validation_loader:\n",
    "        image_batch = image_batch.to(device)\n",
    "        labels_batch = labels_batch.to(device)\n",
    "        pred = model(image_batch)\n",
    "        test_loss += loss_function(pred, labels_batch).item()\n",
    "        correct += (pred.argmax(1) == labels_batch).type(torch.float).sum().item()\n",
    "correct /= size\n",
    "test_loss /= size\n",
    "print(f\"Accuracy: {(100*correct):>0.1f}%, Loss: {test_loss:>8f} \\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
